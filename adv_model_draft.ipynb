{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Advanced Model\n",
    "\n",
    "Divam Arora, Connor Moore, Hemanth Velan\n",
    "\n",
    "DSBA 6165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import nltk\n",
    "import random\n",
    "import keras\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from evaluate import load\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Because our dataset is pulled directly from Huggingface's datasets library, there is no need for a local copy of the data. Running the cell below creates an instance of the specified dataset in your workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 3803957\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 1951\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 189651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df exported.\n",
      "Test df exported.\n",
      "Validation df exported.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the train-test split\n",
    "The standard provided division between train, test, and validation is extremely unbalanced towards train (95%), and the dataset overall is far too large to run through our model in a reasonable timespan. We decided to shrink the train set to 70,000 entries, and concat the provided test and validation sets. From that combined test-val set we will extract a 25,000-entry test set and a 5,000 entry validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>a british soldier was killed saturday by an ex...</td>\n",
       "      <td>british soldier killed in afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>ukraine insists on building two new nuclear re...</td>\n",
       "      <td>ukraine insists on linking chernobyl closure t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>portuguese president mario soares will pay an ...</td>\n",
       "      <td>portugal 's president to visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>aol stepped up its transformation from interne...</td>\n",
       "      <td>aol introduces new advertising network plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>marine experts from wwf flew to the northern k...</td>\n",
       "      <td>suspected toxic algae bloom leaves thousands o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124694</th>\n",
       "      <td>hong kong 's benchmark hang seng index ended h...</td>\n",
       "      <td>hong kong stocks edged up after four straight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237703</th>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira says he 's close to an agreement to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671101</th>\n",
       "      <td>around ## youths on thursday protested outside...</td>\n",
       "      <td>latvian youths protest ban of UNK symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601285</th>\n",
       "      <td>ohio 's method of putting prisoners to death i...</td>\n",
       "      <td>ohio judge says state s lethal injection proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716533</th>\n",
       "      <td>yi zhang and lin xing made a #-# chinese finis...</td>\n",
       "      <td>china 's yi wins women 's triathlon at asian b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   a british soldier was killed saturday by an ex...   \n",
       "1506983  ukraine insists on building two new nuclear re...   \n",
       "3429980  portuguese president mario soares will pay an ...   \n",
       "2028209  aol stepped up its transformation from interne...   \n",
       "1392922  marine experts from wwf flew to the northern k...   \n",
       "...                                                    ...   \n",
       "3124694  hong kong 's benchmark hang seng index ended h...   \n",
       "1237703  former brazil coach carlos alberto parreira sa...   \n",
       "671101   around ## youths on thursday protested outside...   \n",
       "1601285  ohio 's method of putting prisoners to death i...   \n",
       "3716533  yi zhang and lin xing made a #-# chinese finis...   \n",
       "\n",
       "                                                   summary  \n",
       "644708         british soldier killed in afghanistan blast  \n",
       "1506983  ukraine insists on linking chernobyl closure t...  \n",
       "3429980   portugal 's president to visit angola next month  \n",
       "2028209  aol introduces new advertising network plans t...  \n",
       "1392922  suspected toxic algae bloom leaves thousands o...  \n",
       "...                                                    ...  \n",
       "3124694  hong kong stocks edged up after four straight ...  \n",
       "1237703  parreira says he 's close to an agreement to c...  \n",
       "671101           latvian youths protest ban of UNK symbols  \n",
       "1601285  ohio judge says state s lethal injection proce...  \n",
       "3716533  china 's yi wins women 's triathlon at asian b...  \n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 70,000 rows randomly from the train dataframe\n",
    "\n",
    "df_train_short = df_train.sample(n = 70000, random_state=2, ignore_index=True)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine provided test and val sets and reseparate randomly into smaller subsets\n",
    "\n",
    "# concat test and validation sets\n",
    "test_val = [df_test, df_val]\n",
    "df_testval_bulk = pd.concat(test_val)\n",
    "\n",
    "# take a random sample of 30000 rows from the test and validation bulk set\n",
    "df_testval_short = df_testval_bulk.sample(n = 30000, random_state=3, ignore_index=True)\n",
    "\n",
    "# take a random 5000 row sample from the test-val subset\n",
    "df_val_short = df_testval_short.sample(n = 5000, random_state=4, ignore_index=True)\n",
    "\n",
    "# drop all rows taken for the validation sample from the test-val subset to create the test set\n",
    "df_test_short = df_testval_short.drop(df_val_short.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=False)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=False)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code and ideas for this process were gathered from the following geeks for geeks page and article -\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "# comments and functions are original work, source code was modified to fit our workspace\n",
    "\n",
    "# tokenization and removal of stopwords\n",
    "\n",
    "# create an instance of all stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function for removing stopwords from a given input\n",
    "def remove_stopwords(text):\n",
    "    # tokenize the input string\n",
    "    tokens = word_tokenize(text)\n",
    "    # create an empty list for the new output\n",
    "    filtered_tokens = []\n",
    "    # for each word in the tokenized text\n",
    "    for word in tokens:\n",
    "        # if the word is not a stop word\n",
    "        if word not in stop_words:\n",
    "            # add the token to the new output list\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# function to apply the stopword removal/tokenization function to input dataframes\n",
    "def tokenize_nostop(df):\n",
    "    # iterate through the dataframe and tokenize/remove stop words for each row\n",
    "    df[\"document\"] = df[\"document\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from document column\")\n",
    "    \n",
    "    df[\"summary\"] = df[\"summary\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from summary column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration and source code for NLTK's word net lemmatizer came from the following article -\n",
    "# https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "# functions and comments are our original work, source code was modified to fit our workspace\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "# create an instance of NLTK's word net lemmatizer class\n",
    "wml = WordNetLemmatizer()\n",
    "\n",
    "# function to lemmatize a given tokenized text input\n",
    "def lemmatization(text):\n",
    "    # create an empty list for new output\n",
    "    lemma_words = []\n",
    "\n",
    "    # for each word in the given input\n",
    "    for word in text:\n",
    "        # lemmatize the word\n",
    "        token = wml.lemmatize(word)\n",
    "        # and add it into our new output list\n",
    "        lemma_words.append(token)\n",
    "    \n",
    "    return lemma_words\n",
    "\n",
    "# function to call lemmatization function on the rows of an input dataframe\n",
    "def lemmatize(df):\n",
    "    # iterate through the rows of the input dataframe and apply the lemmatization function to each row\n",
    "    df[\"document\"] = df[\"document\"].apply(lemmatization)\n",
    "    print(\"document column lemmatized\")\n",
    "\n",
    "    df[\"summary\"] = df[\"summary\"].apply(lemmatization)\n",
    "    print(\"summary column lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    # tokenize and remove stopwords\n",
    "    tokenize_nostop(df)\n",
    "    # lemmatize\n",
    "    lemmatize(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "train df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "test df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "validation df completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887130</th>\n",
       "      <td>[indian, share, price, closed, percent, higher...</td>\n",
       "      <td>[indian, share, close, pct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230066</th>\n",
       "      <td>[gustav, slammed, cuba, tobaccogrowing, wester...</td>\n",
       "      <td>[gustav, slam, cuba, massive, category, hurric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933565</th>\n",
       "      <td>[week, ago, researcher, wisconsin, japan, said...</td>\n",
       "      <td>[unk, stem, cell, venture, land, million]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592999</th>\n",
       "      <td>[two, japan, biggest, soccer, star, returned, ...</td>\n",
       "      <td>[soccer, star, return, home, dropped, national...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960389</th>\n",
       "      <td>[united, state, britain, unleashed, massive, a...</td>\n",
       "      <td>[u, unleashes, aerial, assault, take, port, ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "887130   [indian, share, price, closed, percent, higher...   \n",
       "1230066  [gustav, slammed, cuba, tobaccogrowing, wester...   \n",
       "2933565  [week, ago, researcher, wisconsin, japan, said...   \n",
       "1592999  [two, japan, biggest, soccer, star, returned, ...   \n",
       "960389   [united, state, britain, unleashed, massive, a...   \n",
       "\n",
       "                                                   summary  \n",
       "644708      [british, soldier, killed, afghanistan, blast]  \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...  \n",
       "3429980  [portugal, president, visit, angola, next, month]  \n",
       "2028209  [aol, introduces, new, advertising, network, p...  \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...  \n",
       "887130                         [indian, share, close, pct]  \n",
       "1230066  [gustav, slam, cuba, massive, category, hurric...  \n",
       "2933565          [unk, stem, cell, venture, land, million]  \n",
       "1592999  [soccer, star, return, home, dropped, national...  \n",
       "960389   [u, unleashes, aerial, assault, take, port, ai...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "pre_proc(df_train_short)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_val_short)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_test_short)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train_short.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert lists to string format for improved model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list entries into single strings\n",
    "\n",
    "def list2string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "      <td>british soldier killed saturday explosion sout...</td>\n",
       "      <td>british soldier killed afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "      <td>ukraine insists linking chernobyl closure buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "      <td>portugal president visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "      <td>aol introduces new advertising network plan mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "      <td>suspected toxic algae bloom leaf thousand fish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "\n",
       "                                                   summary  \\\n",
       "644708      [british, soldier, killed, afghanistan, blast]   \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...   \n",
       "3429980  [portugal, president, visit, angola, next, month]   \n",
       "2028209  [aol, introduces, new, advertising, network, p...   \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...   \n",
       "\n",
       "                                                 docString  \\\n",
       "644708   british soldier killed saturday explosion sout...   \n",
       "1506983  ukraine insists building two new nuclear react...   \n",
       "3429980  portuguese president mario soares pay official...   \n",
       "2028209  aol stepped transformation internet access pro...   \n",
       "1392922  marine expert wwf flew northern kenyan coast t...   \n",
       "\n",
       "                                                 sumString  \n",
       "644708            british soldier killed afghanistan blast  \n",
       "1506983  ukraine insists linking chernobyl closure buil...  \n",
       "3429980         portugal president visit angola next month  \n",
       "2028209  aol introduces new advertising network plan mo...  \n",
       "1392922  suspected toxic algae bloom leaf thousand fish...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply function to create new stringified columns for\n",
    "\n",
    "# train\n",
    "df_train_short[\"docString\"] = df_train_short[\"document\"].map(list2string)\n",
    "df_train_short[\"sumString\"] = df_train_short[\"summary\"].map(list2string)\n",
    "\n",
    "# test\n",
    "df_test_short[\"docString\"] = df_test_short[\"document\"].map(list2string)\n",
    "df_test_short[\"sumString\"] = df_test_short[\"summary\"].map(list2string)\n",
    "\n",
    "# and val\n",
    "df_val_short[\"docString\"] = df_val_short[\"document\"].map(list2string)\n",
    "df_val_short[\"sumString\"] = df_val_short[\"summary\"].map(list2string)\n",
    "\n",
    "df_train_short.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv model writeup goes here\n",
    "(from canvas) \"Write about how your advanced model is different from your baseline model. Why did you choose the model architecture ? What evidence from the previous model milestone did you use to drive your decision making? Write at least 100 words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for adv model goes here and in cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BART-large tokenizer and model\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mini train dataset for testing\n",
    "df_train_prac = df_train_short.sample(n=5, random_state=5, ignore_index=True)\n",
    "\n",
    "df_train_prac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BART_pred(doc):\n",
    "    # set max length for output summary based on input doc length\n",
    "    maxLen = int(len(doc) / 10) + 2\n",
    "\n",
    "    # tokenize input and pass to model\n",
    "    input = tokenizer.batch_encode_plus([doc], return_tensors='pt')\n",
    "    summary_id =  model.generate(input['input_ids'], max_length=maxLen, min_length=1)\n",
    "\n",
    "    # decode summary\n",
    "    summary = tokenizer.decode(summary_id[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original doc: myanmar saw increased annual copper production ton country unk copper mine according local myanmar time monday\n",
      "predicted sum: myanmar saw increased annual copper production ton country un\n",
      "target sum: myanmar see higher copper production\n",
      "\n",
      "original doc: usc coach john robinson problem think quarterback rob johnson unable win big game\n",
      "predicted sum: usc coach john robinson problem think\n",
      "target sum: usc robinson say qb johnson come big occasion\n",
      "\n",
      "original doc: majority leader house fine job majority exists behind bill\n",
      "predicted sum: majority leader says he\n",
      "target sum: house majority leader quest consensus\n",
      "\n",
      "original doc: heavy downpors failed dampen spirit first day london notting hill carnival sign serious street violence marred last year event\n",
      "predicted sum: heavy downpors failed dampen spirit first day l\n",
      "target sum: heavy rain little violence london carnival\n",
      "\n",
      "original doc: henry kaufman president u financial consultant company henry kaufman amp co said dollar could rise high yen yen strength u economic growth\n",
      "predicted sum: henry kaufman president u financial consultant company henry\n",
      "target sum: u finance consultant kaufman say dollar may rise yen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_train_prac)):\n",
    "    print(\"original doc:\", df_train_prac.iloc[i][\"docString\"])\n",
    "    input_doc = df_train_prac.iloc[i][\"docString\"]\n",
    "    pred = BART_pred(input_doc)\n",
    "    print(\"predicted sum:\", pred)\n",
    "    print(\"target sum:\", df_train_prac.iloc[i][\"sumString\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training adv model using train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adv model to generate predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating metrics on adv model test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model performance results writeup goes here\n",
    "(from canvas) \"You have been able to create a training and testing set from your data (or it has already been given to you). We want to see evidence that you were able to train your advanced model and have performance metrics. How does your model perform on the metrics you have chosen from your previous submission?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
