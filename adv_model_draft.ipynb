{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Advanced Model\n",
    "\n",
    "Divam Arora, Connor Moore, Hemanth Velan\n",
    "\n",
    "DSBA 6165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import evaluate\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from evaluate import load\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Because our dataset is pulled directly from Huggingface's datasets library, there is no need for a local copy of the data. Running the cell below creates an instance of the specified dataset in your workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the train-test split\n",
    "The standard provided division between train, test, and validation is extremely unbalanced towards train (95%), and the dataset overall is far too large to run through our model in a reasonable timespan. We decided to shrink the train set to 70,000 entries, and concat the provided test and validation sets. From that combined test-val set we will extract a 25,000-entry test set and a 5,000 entry validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 70,000 rows randomly from the train dataframe\n",
    "\n",
    "df_train_short = df_train.sample(n = 70000, random_state=2, ignore_index=True)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine provided test and val sets and reseparate randomly into smaller subsets\n",
    "\n",
    "# concat test and validation sets\n",
    "test_val = [df_test, df_val]\n",
    "df_testval_bulk = pd.concat(test_val)\n",
    "\n",
    "# take a random sample of 30000 rows from the test and validation bulk set\n",
    "df_testval_short = df_testval_bulk.sample(n = 30000, random_state=3, ignore_index=True)\n",
    "\n",
    "# take a random 5000 row sample from the test-val subset\n",
    "df_val_short = df_testval_short.sample(n = 5000, random_state=4, ignore_index=True)\n",
    "\n",
    "# drop all rows taken for the validation sample from the test-val subset to create the test set\n",
    "df_test_short = df_testval_short.drop(df_val_short.index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "We decided to truncate our pre-processing pipeline slightly from our original model and submission because BART models are designed to accept full, grammatically correct sentences, so we thought passing more \"normal\" text may give the model better context and improve training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=False)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=False)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "pre_proc(df_train_short)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_val_short)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_test_short)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train_short.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv model writeup goes here\n",
    "(from canvas) \"Write about how your advanced model is different from your baseline model. Why did you choose the model architecture ? What evidence from the previous model milestone did you use to drive your decision making? Write at least 100 words.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to optimize model using this article - https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef\n",
    "\n",
    "and this notebook - https://colab.research.google.com/drive/1Cy27V-7qqYatqMA7fEqG2kgMySZXw9I4?usp=sharing&pli=1#scrollTo=t77cjYY_fZlb\n",
    "\n",
    "everything in cells below is from that source notebook, i modified it to fit our workspace and changed some of the parameters/functions to make sense for our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids, pad_token_id):\n",
    "  \"\"\" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n",
    "      This is taken directly from modeling_bart.py\n",
    "  \"\"\"\n",
    "  prev_output_tokens = input_ids.clone()\n",
    "  index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "  prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
    "  prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "  return prev_output_tokens\n",
    "\n",
    "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=32, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "  ''' Function that tokenizes a sentence \n",
    "      Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences\n",
    "      Returns: Dictionary with keys: input_ids, attention_mask, target_ids\n",
    "  '''\n",
    "\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  target_ids = []\n",
    "  tokenized_sentences = {}\n",
    "\n",
    "  for sentence in source_sentences:\n",
    "    encoded_dict = tokenizer(\n",
    "          sentence,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "      )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  input_ids = torch.cat(input_ids, dim = 0)\n",
    "  attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "  for sentence in target_sentences:\n",
    "    encoded_dict = tokenizer(\n",
    "          sentence,\n",
    "          max_length=max_length,\n",
    "          padding=\"max_length\" if pad_to_max_length else None,\n",
    "          truncation=True,\n",
    "          return_tensors=return_tensors,\n",
    "          add_prefix_space = True\n",
    "      )\n",
    "    # Shift the target ids to the right\n",
    "    # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "    target_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "  target_ids = torch.cat(target_ids, dim = 0)\n",
    "  \n",
    "\n",
    "  batch = {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      \"labels\": target_ids,\n",
    "  }\n",
    "\n",
    "  return batch\n",
    "\n",
    "\n",
    "def noise_sentence(sentence_, percent_words, replacement_token = \"<mask>\"):\n",
    "  '''\n",
    "  Function that noises a sentence by adding <mask> tokens\n",
    "  Args: sentence - the sentence to noise\n",
    "        percent_words - the percent of words to replace with <mask> tokens; the number is rounded up using math.ceil\n",
    "  Returns a noised sentence\n",
    "  '''\n",
    "  # Create a list item and copy\n",
    "  sentence_ = sentence_.split(' ')\n",
    "  sentence = sentence_.copy()\n",
    "  \n",
    "  num_words = math.ceil(len(sentence) * percent_words)\n",
    "  \n",
    "  # Create an array of tokens to sample from; don't include the last word as an option because in the case of lyrics\n",
    "  # that word is often a rhyming word and plays an important role in song construction\n",
    "  sample_tokens = set(np.arange(0, np.maximum(1, len(sentence)-1)))\n",
    "  \n",
    "  words_to_noise = random.sample(sample_tokens, num_words)\n",
    "  \n",
    "  # Swap out words, but not full stops\n",
    "  for pos in words_to_noise:\n",
    "      if sentence[pos] != '.':\n",
    "          sentence[pos] = replacement_token\n",
    "  \n",
    "  # Remove redundant spaces\n",
    "  sentence = re.sub(r' {2,5}', ' ', ' '.join(sentence))\n",
    "  \n",
    "  # Combine concurrent <mask> tokens into a single token; this just does two rounds of this; more could be done\n",
    "  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n",
    "  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n",
    "  return sentence\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the hparams dictionary to pass in the model\n",
    "# I realise that this isn't really how this is meant to be used, but having this here reminds me that I can edit it when I need\n",
    "params = argparse.Namespace()\n",
    "\n",
    "params.freeze_encoder = True\n",
    "params.freeze_embeds = True\n",
    "params.eval_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "  # Instantiate the model\n",
    "  def __init__(self, learning_rate, tokenizer, model, params):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.model = model\n",
    "    self.learning_rate = learning_rate\n",
    "    # self.freeze_encoder = freeze_encoder\n",
    "    # self.freeze_embeds_ = freeze_embeds\n",
    "    self.params = params\n",
    "\n",
    "    if self.params.freeze_encoder:\n",
    "      freeze_params(self.model.get_encoder())\n",
    "\n",
    "    if self.params.freeze_embeds:\n",
    "      self.freeze_embeds()\n",
    "  \n",
    "  def freeze_embeds(self):\n",
    "    ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n",
    "    freeze_params(self.model.model.shared)\n",
    "    for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "      freeze_params(d.embed_positions)\n",
    "      freeze_params(d.embed_tokens)\n",
    "\n",
    "  # Do a forward pass through the model\n",
    "  def forward(self, input_ids, **kwargs):\n",
    "    return self.model(input_ids, **kwargs)\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    # Load the data into variables\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "    # Shift the decoder tokens right (but NOT the tgt_ids)\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "\n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "    # Create the loss function\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    # Calculate the loss on the un-shifted tokens\n",
    "    loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss':loss}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "\n",
    "    src_ids, src_mask = batch[0], batch[1]\n",
    "    tgt_ids = batch[2]\n",
    "\n",
    "    decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Run the model and get the logits\n",
    "    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "    lm_logits = outputs[0]\n",
    "\n",
    "    ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "    val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
    "\n",
    "    return {'loss': val_loss}\n",
    "  \n",
    "  # Method that generates text using the BartForConditionalGeneration's generate() method\n",
    "  def generate_text(self, inputs, max_length, min_length, length_penalty, num_beams, early_stopping=True):\n",
    "    ''' Function to generate text '''\n",
    "    generated_id = self.model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=length_penalty, num_beams=num_beams, early_stopping=early_stopping)\n",
    "    return generated_id\n",
    "\n",
    "def freeze_params(model):\n",
    "  ''' Function that takes a model as input (or part of a model) and freezes the layers for faster training\n",
    "      adapted from finetune.py '''\n",
    "  for layer in model.parameters():\n",
    "    layer.requires_grade = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloading module as per the PyTorch Lightning Docs\n",
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "  def __init__(self, tokenizer, train, test, validate, batch_size):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.batch_size = batch_size\n",
    "    self.train = train\n",
    "    self.test = test\n",
    "    self.validate = validate\n",
    "\n",
    "  # encode the sentences using the tokenizer  \n",
    "  def setup(self, stage):\n",
    "    self.train = encode_sentences(self.tokenizer, self.train['document'], self.train['summary'])\n",
    "    self.validate = encode_sentences(self.tokenizer, self.validate['document'], self.validate['summary'])\n",
    "    self.test = encode_sentences(self.tokenizer, self.test['document'], self.test['summary'])\n",
    "\n",
    "  # Load the training, validation and test sets in Pytorch Dataset objects\n",
    "  def train_dataloader(self):\n",
    "    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
    "    train_data = DataLoader(dataset, num_workers=7, persistent_workers=True, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
    "    return train_data\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n",
    "    val_data = DataLoader(dataset, num_workers=7, persistent_workers=True, batch_size = self.batch_size)                       \n",
    "    return val_data\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n",
    "    test_data = DataLoader(dataset, num_workers=7, persistent_workers=True, batch_size = self.batch_size)                   \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into the model for training\n",
    "summary_data = SummaryDataModule(tokenizer, df_train_short, df_test_short, df_val_short, batch_size = 16)\n",
    "\n",
    "model = LitModel(learning_rate=2e-5, tokenizer=tokenizer, model=bart_model, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=False,\n",
    "                     max_epochs = 1,\n",
    "                     min_epochs = 1,\n",
    "                     enable_model_summary=True,\n",
    "                     enable_progress_bar=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the instantiated model to the data\n",
    "trainer.fit(model, summary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of using that source notebook - trying out the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW VERSION OF RUNBART with no NA outputs\n",
    "\n",
    "\n",
    "def runBart(df):\n",
    "\n",
    "    # Empty lists for predictions and performance timestamps\n",
    "    predictions = []\n",
    "    times = []\n",
    "\n",
    "    # For the number of rows in the given dataframe\n",
    "    for i in range(len(df)):\n",
    "        # Create a start timestamp\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Create a document instance using the row's entry for the stringified document\n",
    "        doc = df.iloc[i][\"document\"]\n",
    "\n",
    "        # Encoding inputs using BART tokenizer \n",
    "        inputs = tokenizer.encode(doc, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate vectorized summary using encoded inputs\n",
    "        summary_ids = model.generate_text(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "        # Decode the summary into a human-readable format\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Append the predicted summary to a list of predictions\n",
    "        predictions.append(summary)\n",
    "\n",
    "        # Create an end timestamp\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Calculate computation speed\n",
    "        speed = end - start\n",
    "\n",
    "        # Append computation speed to list\n",
    "        times.append(speed)\n",
    "\n",
    "        # If the iteration is a multiple of 1000\n",
    "        if i % 5000 == 0:\n",
    "            # Calculate the average computation time per row so far and print\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(\"Average time per row at\", i, \"row:\", avg_time)\n",
    "\n",
    "    # Create a new column for the dataframe using the predictions generated and return the modified dataframe\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runBart(df_test_short)\n",
    "\n",
    "df_test_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize BERTScore metric\n",
    "\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating BERTScore metrics for model predictions\n",
    "\n",
    "# create list of prediction outputs\n",
    "test_predictions = list(df_test_short[\"BART_Pred\"].astype(str))\n",
    "# create list of true outputs\n",
    "test_references = list(df_test_short[\"summary\"].astype(str))\n",
    "# calculate BERTScore values comparing model predictions with true summaries\n",
    "test_results_bert = bertscore.compute(predictions=test_predictions, references=test_references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average precision, recall and F1 scores from BERTScore model based on model predictions\n",
    "\n",
    "# create list of result keys\n",
    "test_keys = list(test_results_bert.keys())\n",
    "\n",
    "# for number of values in keylist-1\n",
    "for k in range(len(test_keys)-1):\n",
    "    # sum total of all result values\n",
    "    s_test = sum(test_results_bert[test_keys[k]])\n",
    "    # calculate the total number of result values\n",
    "    le_test = len(test_results_bert[test_keys[k]])\n",
    "    # compute average result value\n",
    "    avg_test = s_test/le_test\n",
    "\n",
    "    print(\"test set result:\")\n",
    "    print(\"Average {} is {}\".format(test_keys[k], avg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ROGUE metrics model\n",
    "\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ROGUE metrics scores for train and test model outputs\n",
    "\n",
    "# compute ROGUE metrics scores comparing model predictions with true outputs\n",
    "test_results_rogue = rouge.compute(predictions=test_predictions, references=test_references)\n",
    "\n",
    "print(\"test set results:\")\n",
    "print(test_results_rogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model performance results writeup goes here\n",
    "(from canvas) \"You have been able to create a training and testing set from your data (or it has already been given to you). We want to see evidence that you were able to train your advanced model and have performance metrics. How does your model perform on the metrics you have chosen from your previous submission?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
