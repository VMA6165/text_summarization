{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Advanced Model\n",
    "\n",
    "Divam Arora, Connor Moore, Hemanth Velan\n",
    "\n",
    "DSBA 6165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from evaluate import load\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Because our dataset is pulled directly from Huggingface's datasets library, there is no need for a local copy of the data. Running the cell below creates an instance of the specified dataset in your workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 3803957\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 1951\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 189651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df exported.\n",
      "Test df exported.\n",
      "Validation df exported.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the train-test split\n",
    "The standard provided division between train, test, and validation is extremely unbalanced towards train (95%), and the dataset overall is far too large to run through our model in a reasonable timespan. We decided to shrink the train set to 70,000 entries, and concat the provided test and validation sets. From that combined test-val set we will extract a 25,000-entry test set and a 5,000 entry validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a british soldier was killed saturday by an ex...</td>\n",
       "      <td>british soldier killed in afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukraine insists on building two new nuclear re...</td>\n",
       "      <td>ukraine insists on linking chernobyl closure t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portuguese president mario soares will pay an ...</td>\n",
       "      <td>portugal 's president to visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aol stepped up its transformation from interne...</td>\n",
       "      <td>aol introduces new advertising network plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marine experts from wwf flew to the northern k...</td>\n",
       "      <td>suspected toxic algae bloom leaves thousands o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>hong kong 's benchmark hang seng index ended h...</td>\n",
       "      <td>hong kong stocks edged up after four straight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira says he 's close to an agreement to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>around ## youths on thursday protested outside...</td>\n",
       "      <td>latvian youths protest ban of UNK symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>ohio 's method of putting prisoners to death i...</td>\n",
       "      <td>ohio judge says state s lethal injection proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>yi zhang and lin xing made a #-# chinese finis...</td>\n",
       "      <td>china 's yi wins women 's triathlon at asian b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      a british soldier was killed saturday by an ex...   \n",
       "1      ukraine insists on building two new nuclear re...   \n",
       "2      portuguese president mario soares will pay an ...   \n",
       "3      aol stepped up its transformation from interne...   \n",
       "4      marine experts from wwf flew to the northern k...   \n",
       "...                                                  ...   \n",
       "69995  hong kong 's benchmark hang seng index ended h...   \n",
       "69996  former brazil coach carlos alberto parreira sa...   \n",
       "69997  around ## youths on thursday protested outside...   \n",
       "69998  ohio 's method of putting prisoners to death i...   \n",
       "69999  yi zhang and lin xing made a #-# chinese finis...   \n",
       "\n",
       "                                                 summary  \n",
       "0            british soldier killed in afghanistan blast  \n",
       "1      ukraine insists on linking chernobyl closure t...  \n",
       "2       portugal 's president to visit angola next month  \n",
       "3      aol introduces new advertising network plans t...  \n",
       "4      suspected toxic algae bloom leaves thousands o...  \n",
       "...                                                  ...  \n",
       "69995  hong kong stocks edged up after four straight ...  \n",
       "69996  parreira says he 's close to an agreement to c...  \n",
       "69997          latvian youths protest ban of UNK symbols  \n",
       "69998  ohio judge says state s lethal injection proce...  \n",
       "69999  china 's yi wins women 's triathlon at asian b...  \n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 70,000 rows randomly from the train dataframe\n",
    "\n",
    "df_train_short = df_train.sample(n = 70000, random_state=2, ignore_index=True)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine provided test and val sets and reseparate randomly into smaller subsets\n",
    "\n",
    "# concat test and validation sets\n",
    "test_val = [df_test, df_val]\n",
    "df_testval_bulk = pd.concat(test_val)\n",
    "\n",
    "# take a random sample of 30000 rows from the test and validation bulk set\n",
    "df_testval_short = df_testval_bulk.sample(n = 30000, random_state=3, ignore_index=True)\n",
    "\n",
    "# take a random 5000 row sample from the test-val subset\n",
    "df_val_short = df_testval_short.sample(n = 5000, random_state=4, ignore_index=True)\n",
    "\n",
    "# drop all rows taken for the validation sample from the test-val subset to create the test set\n",
    "df_test_short = df_testval_short.drop(df_val_short.index, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "We decided to truncate our pre-processing pipeline slightly from our baseline model notebook (removing lemmatization and not vectorizing text prior to passing it to the BART pretrained tokenizer) because BART models are designed to accept full, grammatically correct sentences. We thought passing more \"normal\" text during training may give the model better context and improve learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=False)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=False)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "pre-processed successfully\n",
      "train df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "pre-processed successfully\n",
      "test df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "pre-processed successfully\n",
      "validation df completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a british soldier was killed saturday by an ex...</td>\n",
       "      <td>british soldier killed in afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ukraine insists on building two new nuclear re...</td>\n",
       "      <td>ukraine insists on linking chernobyl closure t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portuguese president mario soares will pay an ...</td>\n",
       "      <td>portugal  president to visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aol stepped up its transformation from interne...</td>\n",
       "      <td>aol introduces new advertising network plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marine experts from wwf flew to the northern k...</td>\n",
       "      <td>suspected toxic algae bloom leaves thousands o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  a british soldier was killed saturday by an ex...   \n",
       "1  ukraine insists on building two new nuclear re...   \n",
       "2  portuguese president mario soares will pay an ...   \n",
       "3  aol stepped up its transformation from interne...   \n",
       "4  marine experts from wwf flew to the northern k...   \n",
       "\n",
       "                                             summary  \n",
       "0        british soldier killed in afghanistan blast  \n",
       "1  ukraine insists on linking chernobyl closure t...  \n",
       "2     portugal  president to visit angola next month  \n",
       "3  aol introduces new advertising network plans t...  \n",
       "4  suspected toxic algae bloom leaves thousands o...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "# minus the lemmatization and tokenizer steps from the baseline notebook\n",
    "\n",
    "pre_proc(df_train_short)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_val_short)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_test_short)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train_short.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New attempt at fine-tuning / advanced model\n",
    "Using same approach utilized in baseline model notebook, with modifications and adjustments made to runBART() function to eliminate null value model output issues and attempt to improve hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and model loading for bart-base\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version OF runBart function, updated from our baseline model for improved speed and performance\n",
    "# testing and trial and error showed that removing the component of the function that calculated a unique maxlength for every summary sped up prediction generation significantly\n",
    "# in the baseline model we had issues with large numbers of NaN empty summary predictions,\n",
    "# through some research, testing, and trial and error we were able to find a configuration for the encode and generate() steps that corrected this issue\n",
    "\n",
    "def runBart(df):\n",
    "\n",
    "    # Empty lists for predictions and performance timestamps\n",
    "    predictions = []\n",
    "    times = []\n",
    "\n",
    "    # For the number of rows in the given dataframe\n",
    "    for i in range(len(df)):\n",
    "        # Create a start timestamp\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Create a document instance using the row's entry for the stringified document\n",
    "        doc = df.iloc[i][\"document\"]\n",
    "\n",
    "        # Encoding inputs using BART tokenizer \n",
    "        inputs = tokenizer.encode(doc, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "        # Generate vectorized summary using encoded inputs\n",
    "        summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "        # Decode the summary into a human-readable format\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Append the predicted summary to a list of predictions\n",
    "        predictions.append(summary)\n",
    "\n",
    "        # Create an end timestamp\n",
    "        end = time.perf_counter()\n",
    "\n",
    "        # Calculate computation speed\n",
    "        speed = end - start\n",
    "\n",
    "        # Append computation speed to list\n",
    "        times.append(speed)\n",
    "\n",
    "        # If the iteration is a multiple of 1000\n",
    "        if i % 1000 == 0:\n",
    "            # Calculate the average computation time per row so far and print\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(\"Average time per row at\", i, \"row:\", avg_time)\n",
    "\n",
    "    # Create a new column for the dataframe using the predictions generated and return the modified dataframe\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per row at 0 row: 2.2704760999999998\n",
      "Average time per row at 1000 row: 2.1695680690309698\n",
      "Average time per row at 2000 row: 2.1347162535732087\n",
      "Average time per row at 3000 row: 2.126662612062643\n",
      "Average time per row at 4000 row: 2.127252925843536\n",
      "Average time per row at 5000 row: 2.1181546087382452\n",
      "Average time per row at 6000 row: 2.1135295082652883\n",
      "Average time per row at 7000 row: 2.11838996016283\n",
      "Average time per row at 8000 row: 2.1159988310586164\n",
      "Average time per row at 9000 row: 2.1139535430063363\n",
      "Average time per row at 10000 row: 2.1138210887711244\n",
      "Average time per row at 11000 row: 2.113529034642307\n",
      "Average time per row at 12000 row: 2.117017731197398\n",
      "Average time per row at 13000 row: 2.1165382607491736\n",
      "Average time per row at 14000 row: 2.1160646754089028\n",
      "Average time per row at 15000 row: 2.1147231192320524\n",
      "Average time per row at 16000 row: 2.1154324689956807\n",
      "Average time per row at 17000 row: 2.114863878195382\n",
      "Average time per row at 18000 row: 2.114798550869374\n",
      "Average time per row at 19000 row: 2.114825520441015\n",
      "Average time per row at 20000 row: 2.1156128655717104\n",
      "Average time per row at 21000 row: 2.115125211032802\n",
      "Average time per row at 22000 row: 2.1148077446070523\n",
      "Average time per row at 23000 row: 2.1149095050215068\n",
      "Average time per row at 24000 row: 2.1141575609599474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>BART_Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>japanese electronics giant toshiba said tuesda...</td>\n",
       "      <td>toshiba profits  times up in third quarter</td>\n",
       "      <td>japanese electronics giant toshiba said tuesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>michael campbell opened a unk lead on defendin...</td>\n",
       "      <td>campbell puts defending champion woosnam in tr...</td>\n",
       "      <td>michael campbell opened a unk lead on defendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>iran on tuesday dismissed the allegation by th...</td>\n",
       "      <td>iran denies allegation on its military deployment</td>\n",
       "      <td>iran on tuesday dismissed the allegation by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>turkish foreign minister and deputy prime mini...</td>\n",
       "      <td>turkish fm hails eu plan to end economic sanct...</td>\n",
       "      <td>turkish foreign minister and deputy prime mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>patricia mcgovern  the former senate ways and ...</td>\n",
       "      <td>former senator joins governor race</td>\n",
       "      <td>patricia mcgovern  the former senate ways and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>three activists are charged with staging an un...</td>\n",
       "      <td>charges against activists could set precedent ...</td>\n",
       "      <td>three activists are charged with staging an un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>a prominent fatah leader and his   yearold son...</td>\n",
       "      <td>father son killed in fatahhamas fight in gaza</td>\n",
       "      <td>a prominent fatah leader and his   yearold son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>trustees at virginia union university  one of ...</td>\n",
       "      <td>wilder in running to head virginia union</td>\n",
       "      <td>trustees at virginia union university  one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>patrouille de france   lrb paf rrb the famous...</td>\n",
       "      <td>french famous unk to present aerobatics shows ...</td>\n",
       "      <td>patrouille de france   lrb paf rrb the famous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>the relentless insurgency in the sunni muslim ...</td>\n",
       "      <td>us troops battle resistance fighters west of b...</td>\n",
       "      <td>the relentless insurgency in the sunni muslim ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "5000   japanese electronics giant toshiba said tuesda...   \n",
       "5001   michael campbell opened a unk lead on defendin...   \n",
       "5002   iran on tuesday dismissed the allegation by th...   \n",
       "5003   turkish foreign minister and deputy prime mini...   \n",
       "5004   patricia mcgovern  the former senate ways and ...   \n",
       "...                                                  ...   \n",
       "29995  three activists are charged with staging an un...   \n",
       "29996  a prominent fatah leader and his   yearold son...   \n",
       "29997  trustees at virginia union university  one of ...   \n",
       "29998   patrouille de france   lrb paf rrb the famous...   \n",
       "29999  the relentless insurgency in the sunni muslim ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "5000          toshiba profits  times up in third quarter   \n",
       "5001   campbell puts defending champion woosnam in tr...   \n",
       "5002   iran denies allegation on its military deployment   \n",
       "5003   turkish fm hails eu plan to end economic sanct...   \n",
       "5004                  former senator joins governor race   \n",
       "...                                                  ...   \n",
       "29995  charges against activists could set precedent ...   \n",
       "29996      father son killed in fatahhamas fight in gaza   \n",
       "29997           wilder in running to head virginia union   \n",
       "29998  french famous unk to present aerobatics shows ...   \n",
       "29999  us troops battle resistance fighters west of b...   \n",
       "\n",
       "                                               BART_Pred  \n",
       "5000   japanese electronics giant toshiba said tuesda...  \n",
       "5001   michael campbell opened a unk lead on defendin...  \n",
       "5002   iran on tuesday dismissed the allegation by th...  \n",
       "5003   turkish foreign minister and deputy prime mini...  \n",
       "5004   patricia mcgovern  the former senate ways and ...  \n",
       "...                                                  ...  \n",
       "29995  three activists are charged with staging an un...  \n",
       "29996  a prominent fatah leader and his   yearold son...  \n",
       "29997  trustees at virginia union university  one of ...  \n",
       "29998   patrouille de france   lrb paf rrb the famous...  \n",
       "29999  the relentless insurgency in the sunni muslim ...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the model to generate predictions using the test set\n",
    "runBart(df_test_short)\n",
    "\n",
    "df_test_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there were 0 empty predictions generated.\n"
     ]
    }
   ],
   "source": [
    "# check generated predictions for NaN values\n",
    "\n",
    "# count null values in BART pred columnm\n",
    "null_predictions = df_test_short['BART_Pred'].isna().sum()\n",
    "\n",
    "print(\"there were\", null_predictions, \"empty predictions generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize BERTScore metric\n",
    "\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# generating BERTScore metrics for model predictions\n",
    "\n",
    "# create list of prediction outputs\n",
    "test_predictions = list(df_test_short[\"BART_Pred\"].astype(str))\n",
    "# create list of true outputs\n",
    "test_references = list(df_test_short[\"summary\"].astype(str))\n",
    "# calculate BERTScore values comparing model predictions with true summaries\n",
    "test_results_bert = bertscore.compute(predictions=test_predictions, references=test_references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set result:\n",
      "Average precision is 0.8153642509675026\n",
      "test set result:\n",
      "Average recall is 0.8942867579483986\n",
      "test set result:\n",
      "Average f1 is 0.8527202885365486\n"
     ]
    }
   ],
   "source": [
    "# calculate average precision, recall and F1 scores from BERTScore model based on model predictions\n",
    "\n",
    "# create list of result keys\n",
    "test_keys = list(test_results_bert.keys())\n",
    "\n",
    "# for number of values in keylist-1\n",
    "for k in range(len(test_keys)-1):\n",
    "    # sum total of all result values\n",
    "    s_test = sum(test_results_bert[test_keys[k]])\n",
    "    # calculate the total number of result values\n",
    "    le_test = len(test_results_bert[test_keys[k]])\n",
    "    # compute average result value\n",
    "    avg_test = s_test/le_test\n",
    "\n",
    "    print(\"test set result:\")\n",
    "    print(\"Average {} is {}\".format(test_keys[k], avg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ROUGE metrics model\n",
    "\n",
    "rouge = load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set results:\n",
      "{'rouge1': 0.2140434424280383, 'rouge2': 0.06928421625683026, 'rougeL': 0.18753636941510993, 'rougeLsum': 0.18756050305883293}\n"
     ]
    }
   ],
   "source": [
    "# generate ROUGE metrics scores for train and test model outputs\n",
    "\n",
    "# compute ROUGE metrics scores comparing model predictions with true outputs\n",
    "test_results_rouge = rouge.compute(predictions=test_predictions, references=test_references)\n",
    "\n",
    "print(\"test set results:\")\n",
    "print(test_results_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe a few specific model predictions for human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21906, 15907, 13640, 12386, 16459]\n",
      "\n",
      "index: 21906\n",
      "original input: malaysians believe their country has an  acute  corruption problem  a watchdog said monday  warning the issue could drive away foreign investors \n",
      "target summary: malaysians believe corruption problem acute\n",
      "model prediction: malaysians believe their country has an  acute  corruption problem  a watchdog said monday  warning the issue could drive away foreign investors  and the countryâ€™s economic growth could be affected by the issue.advertisementadvertisementadvertisement advertisement\n",
      "\n",
      "index: 15907\n",
      "original input: kohlberg kravis roberts amp co said it will invest   million in randalls food markets  getting a majority stake in the closely held grocerystore chain \n",
      "target summary: kkr to invest   mln in texas grocer unk food markets\n",
      "model prediction: kohlberg kravis roberts amp co said it will invest   million in randalls food markets  getting a majority stake in the closely held grocerystore chain  and is expected to raise about $1 billion in the coming year.\n",
      "\n",
      "index: 13640\n",
      "original input: the french consumer products group bic said thursday that group sales jumped by  percent to  billion euros lrb  billion dollars rrb last year  owing to stronger demand for its throwaway ballpoint pens  razors and cigarette lighters \n",
      "target summary: bic  sales rise  pct to  billion euros\n",
      "model prediction: the french consumer products group bic said thursday that group sales jumped by  percent to  billion euros lrb  billion dollars rrb last year  owing to stronger demand for its throwaway ballpoint pens  razors and cigarette lighters \n",
      "\n",
      "index: 12386\n",
      "original input: essendon survived an early scare to beat st kilda by  points tuesday in the completion of a game suspended by a blackout at waverley stadium \n",
      "target summary: essendon wins suspended game\n",
      "model prediction: essendon survived an early scare to beat st kilda by  points tuesday in the completion of a game suspended by a blackout at waverley stadium. (0:30)    TURNOVERSEAS:\n",
      "\n",
      "index: 16459\n",
      "original input: the man who led the sniper investigation has been showered by a grateful public with gifts and flowers  and even has a fan web site that declares him  a total superhero \n",
      "target summary: web site cards gifts express admiration for sniper investigator with unk shootings\n",
      "model prediction: the man who led the sniper investigation has been showered by a grateful public with gifts and flowers  and even has a fan web site that declares him  a total superhero  and the man who shot and killed Osama bin Laden has become a hero \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# produce five random input/target/prediction pairs from the model\n",
    "\n",
    "# empty list for index sample\n",
    "indexList = []\n",
    "\n",
    "# generate five random index values and add to list\n",
    "for i in range(5):\n",
    "    index = random.randint(0,25000)\n",
    "    indexList.append(index)\n",
    "\n",
    "print(indexList)\n",
    "print()\n",
    "\n",
    "# for index in list\n",
    "for ind in indexList:\n",
    "    # display the original input, the target summary, and what the model predicted for a summary\n",
    "    print(\"index:\", ind)\n",
    "    print(\"original input:\", df_test_short.iloc[ind][\"document\"])\n",
    "    print(\"target summary:\", df_test_short.iloc[ind][\"summary\"])\n",
    "    print(\"model prediction:\", df_test_short.iloc[ind][\"BART_Pred\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways from new attempt\n",
    "\n",
    "\n",
    "This version of the model performed slightly better than our original advanced model, but not better than our original baseline model. This version generated no null summaries, which is good, but looking at the example outputs it appears part of our problem may be related to our data. Many of BART's generated summaries are longer than our original input sentences, and we think this may be because BART is designed to produce coherent, human-readable sentences. Becuase many of our input strings are shortened or abbreviated sentences (and some are not sentences at all), we wondered if it is possible BART is having a difficult time writing logical sentences that are any shorter (the entire purpose of summarization). Going into this project, none of us had experience with text summarization, but over the course of the semester it has become apparent text summarization models appear to perform better with larger bodies of input text. A takeaway or modification to our project would be to utilize a dataset with longer input text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
