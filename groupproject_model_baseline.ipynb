{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Model Baseline\n",
    "### DSBA 6165\n",
    "### Divam Arora, Connor Moore, Hemanth Velan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "* https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import datasets as ds\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://huggingface.co/datasets/gigaword\n",
    "# # https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# # download gigaword dataset from Hugging Face dataset library\n",
    "# train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display the dataset splits\n",
    "# print(train)\n",
    "# print(test)\n",
    "# print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# # export the training dataset to a pandas dataframe and display\n",
    "# df_train = train.to_pandas()\n",
    "# print(\"Train df exported.\")\n",
    "\n",
    "# # export the test dataset to a pandas dataframe\n",
    "# df_test = test.to_pandas()\n",
    "# print(\"Test df exported.\")\n",
    "\n",
    "# # export the validation dataset to a pandas dataframe\n",
    "# df_val = validation.to_pandas()\n",
    "# print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the methods required to perform this function were found in this article -\n",
    "# # https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# # the function and comments are our original work\n",
    "\n",
    "# # set all words in all rows to lower case\n",
    "\n",
    "# def lower(df):\n",
    "#     # vectorize strings in each row in summary column and set to lower case\n",
    "#     df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "#     print(\"summary column lowercased\")\n",
    "#     # vectorize strings in each row in document column and set to lower case\n",
    "#     df[\"document\"] = df[\"document\"].str.lower()\n",
    "#     print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# # stackoverflow posts helped with debugging issues\n",
    "# # https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# # https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# # https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# # https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# # comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# # remove all symbols and punctuation\n",
    "\n",
    "# # create instance of all punctuation symbols\n",
    "# punctuation = string.punctuation\n",
    "\n",
    "# # since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "# punct_list = [\"'s\"]\n",
    "\n",
    "# # add all punctuation from the premade variable to our new list\n",
    "# for symbol in punctuation:\n",
    "#     punct_list.append(symbol)\n",
    "\n",
    "# # display the symbols included in our list\n",
    "# print(punct_list)\n",
    "\n",
    "# def remove_punctuation(df):\n",
    "#     # for each symbol in our punctuation list\n",
    "#     for symbol in punct_list:\n",
    "#         # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "#         df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=True)\n",
    "#         df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=True)\n",
    "#     print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # source code and ideas for this process were gathered from the following geeks for geeks page and article -\n",
    "# # https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# # https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "# # comments and functions are original work, source code was modified to fit our workspace\n",
    "\n",
    "# # tokenization and removal of stopwords\n",
    "\n",
    "# # create an instance of all stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # function for removing stopwords from a given input\n",
    "# def remove_stopwords(text):\n",
    "#     # tokenize the input string\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # create an empty list for the new output\n",
    "#     filtered_tokens = []\n",
    "#     # for each word in the tokenized text\n",
    "#     for word in tokens:\n",
    "#         # if the word is not a stop word\n",
    "#         if word not in stop_words:\n",
    "#             # add the token to the new output list\n",
    "#             filtered_tokens.append(word)\n",
    "\n",
    "#     return filtered_tokens\n",
    "\n",
    "# # function to apply the stopword removal/tokenization function to input dataframes\n",
    "# def tokenize_nostop(df):\n",
    "#     # iterate through the dataframe and tokenize/remove stop words for each row\n",
    "#     df[\"document\"] = df[\"document\"].apply(remove_stopwords)\n",
    "#     print(\"stopwords removed from document column\")\n",
    "    \n",
    "#     df[\"summary\"] = df[\"summary\"].apply(remove_stopwords)\n",
    "#     print(\"stopwords removed from summary column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inspiration and source code for NLTK's word net lemmatizer came from the following article -\n",
    "# # https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "# # functions and comments are our original work, source code was modified to fit our workspace\n",
    "\n",
    "# # lemmatization\n",
    "\n",
    "# # create an instance of NLTK's word net lemmatizer class\n",
    "# wml = WordNetLemmatizer()\n",
    "\n",
    "# # function to lemmatize a given tokenized text input\n",
    "# def lemmatization(text):\n",
    "#     # create an empty list for new output\n",
    "#     lemma_words = []\n",
    "\n",
    "#     # for each word in the given input\n",
    "#     for word in text:\n",
    "#         # lemmatize the word\n",
    "#         token = wml.lemmatize(word)\n",
    "#         # and add it into our new output list\n",
    "#         lemma_words.append(token)\n",
    "    \n",
    "#     return lemma_words\n",
    "\n",
    "# # function to call lemmatization function on the rows of an input dataframe\n",
    "# def lemmatize(df):\n",
    "#     # iterate through the rows of the input dataframe and apply the lemmatization function to each row\n",
    "#     df[\"document\"] = df[\"document\"].apply(lemmatization)\n",
    "#     print(\"document column lemmatized\")\n",
    "\n",
    "#     df[\"summary\"] = df[\"summary\"].apply(lemmatization)\n",
    "#     print(\"summary column lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create data pre-processing pipeline\n",
    "\n",
    "# def pre_proc(df):\n",
    "#     # lowercase\n",
    "#     lower(df)\n",
    "#     # remove punctuation and symbols\n",
    "#     remove_punctuation(df)\n",
    "#     # tokenize and remove stopwords\n",
    "#     tokenize_nostop(df)\n",
    "#     # lemmatize\n",
    "#     lemmatize(df)\n",
    "#     print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "# pre_proc(df_train)\n",
    "# print(\"train df completed\")\n",
    "# pre_proc(df_test)\n",
    "# print(\"test df completed\")\n",
    "# pre_proc(df_val)\n",
    "# print(\"validation df completed\")\n",
    "\n",
    "# # display new format of data using training set\n",
    "# df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export processed datasets to csv files so that pre-processing does not have to be re-run\n",
    "\n",
    "# df_test.to_csv(\"test.csv\", index=False)\n",
    "# print(\"test set exported\")\n",
    "\n",
    "# df_val.to_csv(\"val.csv\", index=False)\n",
    "# print(\"validation set exported\")\n",
    "\n",
    "# df_train.to_csv(\"train.csv\", index=False)\n",
    "# print(\"train set exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set imported\n",
      "test set imported\n",
      "validation set imported\n"
     ]
    }
   ],
   "source": [
    "# import dataframes from saved csv files\n",
    "\n",
    "df_train = pd.read_csv(\"./Data/train.csv\")\n",
    "print(\"train set imported\")\n",
    "\n",
    "df_test = pd.read_csv(\"./Data/test.csv\")\n",
    "print(\"test set imported\")\n",
    "\n",
    "df_val = pd.read_csv(\"./Data/val.csv\")  \n",
    "print(\"validation set imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['australia', 'current', 'account', 'deficit',...</td>\n",
       "      <td>['australian', 'current', 'account', 'deficit'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['least', 'two', 'people', 'killed', 'suspecte...</td>\n",
       "      <td>['least', 'two', 'dead', 'southern', 'philippi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['australian', 'share', 'closed', 'percent', '...</td>\n",
       "      <td>['australian', 'stock', 'close', 'percent']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['south', 'korea', 'nuclear', 'envoy', 'kim', ...</td>\n",
       "      <td>['envoy', 'urge', 'north', 'korea', 'restart',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['south', 'korea', 'monday', 'announced', 'swe...</td>\n",
       "      <td>['skorea', 'announces', 'tax', 'cut', 'stimula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803952</th>\n",
       "      <td>['state', 'duma', 'lower', 'house', 'russian',...</td>\n",
       "      <td>['duma', 'urge', 'yeltsin', 'reconsider', 'tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803953</th>\n",
       "      <td>['u', 'justice', 'department', 'today', 'rejec...</td>\n",
       "      <td>['u', 'justice', 'department', 'reject', 'spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803954</th>\n",
       "      <td>['united', 'nation', 'calling', 'million', 'do...</td>\n",
       "      <td>['un', 'seek', 'fund', 'program', 'former', 'y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803955</th>\n",
       "      <td>['president', 'jacques', 'chirac', 'today', 'p...</td>\n",
       "      <td>['chirac', 'get', 'birthday', 'gift', 'th', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803956</th>\n",
       "      <td>['united', 'nation', 'tribunal', 'war', 'crime...</td>\n",
       "      <td>['war', 'crime', 'tribunal', 'pass', 'st', 'se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3803957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "0        ['australia', 'current', 'account', 'deficit',...   \n",
       "1        ['least', 'two', 'people', 'killed', 'suspecte...   \n",
       "2        ['australian', 'share', 'closed', 'percent', '...   \n",
       "3        ['south', 'korea', 'nuclear', 'envoy', 'kim', ...   \n",
       "4        ['south', 'korea', 'monday', 'announced', 'swe...   \n",
       "...                                                    ...   \n",
       "3803952  ['state', 'duma', 'lower', 'house', 'russian',...   \n",
       "3803953  ['u', 'justice', 'department', 'today', 'rejec...   \n",
       "3803954  ['united', 'nation', 'calling', 'million', 'do...   \n",
       "3803955  ['president', 'jacques', 'chirac', 'today', 'p...   \n",
       "3803956  ['united', 'nation', 'tribunal', 'war', 'crime...   \n",
       "\n",
       "                                                   summary  \n",
       "0        ['australian', 'current', 'account', 'deficit'...  \n",
       "1        ['least', 'two', 'dead', 'southern', 'philippi...  \n",
       "2              ['australian', 'stock', 'close', 'percent']  \n",
       "3        ['envoy', 'urge', 'north', 'korea', 'restart',...  \n",
       "4        ['skorea', 'announces', 'tax', 'cut', 'stimula...  \n",
       "...                                                    ...  \n",
       "3803952  ['duma', 'urge', 'yeltsin', 'reconsider', 'tro...  \n",
       "3803953  ['u', 'justice', 'department', 'reject', 'spec...  \n",
       "3803954  ['un', 'seek', 'fund', 'program', 'former', 'y...  \n",
       "3803955  ['chirac', 'get', 'birthday', 'gift', 'th', 'c...  \n",
       "3803956  ['war', 'crime', 'tribunal', 'pass', 'st', 'se...  \n",
       "\n",
       "[3803957 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert everything to string for better compatibility with text summarization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all2String(samp_li):\n",
    "    return samp_li.replace(\"\\'\", \"\").strip('][').replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['australia', 'current', 'account', 'deficit',...</td>\n",
       "      <td>['australian', 'current', 'account', 'deficit'...</td>\n",
       "      <td>australia current account deficit shrunk recor...</td>\n",
       "      <td>australian current account deficit narrow sharply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['least', 'two', 'people', 'killed', 'suspecte...</td>\n",
       "      <td>['least', 'two', 'dead', 'southern', 'philippi...</td>\n",
       "      <td>least two people killed suspected bomb attack ...</td>\n",
       "      <td>least two dead southern philippine blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['australian', 'share', 'closed', 'percent', '...</td>\n",
       "      <td>['australian', 'stock', 'close', 'percent']</td>\n",
       "      <td>australian share closed percent monday followi...</td>\n",
       "      <td>australian stock close percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['south', 'korea', 'nuclear', 'envoy', 'kim', ...</td>\n",
       "      <td>['envoy', 'urge', 'north', 'korea', 'restart',...</td>\n",
       "      <td>south korea nuclear envoy kim sook urged north...</td>\n",
       "      <td>envoy urge north korea restart nuclear disable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['south', 'korea', 'monday', 'announced', 'swe...</td>\n",
       "      <td>['skorea', 'announces', 'tax', 'cut', 'stimula...</td>\n",
       "      <td>south korea monday announced sweeping tax refo...</td>\n",
       "      <td>skorea announces tax cut stimulate economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  ['australia', 'current', 'account', 'deficit',...   \n",
       "1  ['least', 'two', 'people', 'killed', 'suspecte...   \n",
       "2  ['australian', 'share', 'closed', 'percent', '...   \n",
       "3  ['south', 'korea', 'nuclear', 'envoy', 'kim', ...   \n",
       "4  ['south', 'korea', 'monday', 'announced', 'swe...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['australian', 'current', 'account', 'deficit'...   \n",
       "1  ['least', 'two', 'dead', 'southern', 'philippi...   \n",
       "2        ['australian', 'stock', 'close', 'percent']   \n",
       "3  ['envoy', 'urge', 'north', 'korea', 'restart',...   \n",
       "4  ['skorea', 'announces', 'tax', 'cut', 'stimula...   \n",
       "\n",
       "                                           docString  \\\n",
       "0  australia current account deficit shrunk recor...   \n",
       "1  least two people killed suspected bomb attack ...   \n",
       "2  australian share closed percent monday followi...   \n",
       "3  south korea nuclear envoy kim sook urged north...   \n",
       "4  south korea monday announced sweeping tax refo...   \n",
       "\n",
       "                                           sumString  \n",
       "0  australian current account deficit narrow sharply  \n",
       "1           least two dead southern philippine blast  \n",
       "2                     australian stock close percent  \n",
       "3  envoy urge north korea restart nuclear disable...  \n",
       "4         skorea announces tax cut stimulate economy  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"docString\"] = df_train[\"document\"].map(all2String)\n",
    "df_train[\"sumString\"] = df_train[\"summary\"].map(all2String)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['japan', 'nec', 'corp', 'unk', 'computer', 'c...</td>\n",
       "      <td>['nec', 'unk', 'computer', 'sale', 'tieup']</td>\n",
       "      <td>japan nec corp unk computer corp united state ...</td>\n",
       "      <td>nec unk computer sale tieup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['sri', 'lankan', 'government', 'wednesday', '...</td>\n",
       "      <td>['sri', 'lanka', 'close', 'school', 'war', 'es...</td>\n",
       "      <td>sri lankan government wednesday announced clos...</td>\n",
       "      <td>sri lanka close school war escalates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['police', 'arrested', 'five', 'antinuclear', ...</td>\n",
       "      <td>['protester', 'target', 'french', 'research', ...</td>\n",
       "      <td>police arrested five antinuclear protester thu...</td>\n",
       "      <td>protester target french research ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['factory', 'order', 'manufactured', 'good', '...</td>\n",
       "      <td>['u', 'september', 'factory', 'order', 'percent']</td>\n",
       "      <td>factory order manufactured good rose percent s...</td>\n",
       "      <td>u september factory order percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['bank', 'japan', 'appealed', 'financial', 'ma...</td>\n",
       "      <td>['bank', 'unk', 'unk', 'calm', 'financial', 'm...</td>\n",
       "      <td>bank japan appealed financial market remain ca...</td>\n",
       "      <td>bank unk unk calm financial market</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  ['japan', 'nec', 'corp', 'unk', 'computer', 'c...   \n",
       "1  ['sri', 'lankan', 'government', 'wednesday', '...   \n",
       "2  ['police', 'arrested', 'five', 'antinuclear', ...   \n",
       "3  ['factory', 'order', 'manufactured', 'good', '...   \n",
       "4  ['bank', 'japan', 'appealed', 'financial', 'ma...   \n",
       "\n",
       "                                             summary  \\\n",
       "0        ['nec', 'unk', 'computer', 'sale', 'tieup']   \n",
       "1  ['sri', 'lanka', 'close', 'school', 'war', 'es...   \n",
       "2  ['protester', 'target', 'french', 'research', ...   \n",
       "3  ['u', 'september', 'factory', 'order', 'percent']   \n",
       "4  ['bank', 'unk', 'unk', 'calm', 'financial', 'm...   \n",
       "\n",
       "                                           docString  \\\n",
       "0  japan nec corp unk computer corp united state ...   \n",
       "1  sri lankan government wednesday announced clos...   \n",
       "2  police arrested five antinuclear protester thu...   \n",
       "3  factory order manufactured good rose percent s...   \n",
       "4  bank japan appealed financial market remain ca...   \n",
       "\n",
       "                               sumString  \n",
       "0            nec unk computer sale tieup  \n",
       "1   sri lanka close school war escalates  \n",
       "2  protester target french research ship  \n",
       "3      u september factory order percent  \n",
       "4     bank unk unk calm financial market  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"docString\"] = df_test[\"document\"].map(all2String)\n",
    "df_test[\"sumString\"] = df_test[\"summary\"].map(all2String)\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['fivetime', 'world', 'champion', 'michelle', ...</td>\n",
       "      <td>['injury', 'leaf', 'kwan', 'olympic', 'hope', ...</td>\n",
       "      <td>fivetime world champion michelle kwan withdrew...</td>\n",
       "      <td>injury leaf kwan olympic hope limbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['u', 'business', 'leader', 'lashed', 'wednesd...</td>\n",
       "      <td>['u', 'business', 'attack', 'tough', 'immigrat...</td>\n",
       "      <td>u business leader lashed wednesday legislation...</td>\n",
       "      <td>u business attack tough immigration law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['general', 'motor', 'corp', 'said', 'wednesda...</td>\n",
       "      <td>['gm', 'december', 'sale', 'fall', 'percent']</td>\n",
       "      <td>general motor corp said wednesday u sale fell ...</td>\n",
       "      <td>gm december sale fall percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['several', 'thousand', 'people', 'gathered', ...</td>\n",
       "      <td>['thousand', 'croatian', 'celebrate', 'world',...</td>\n",
       "      <td>several thousand people gathered wednesday eve...</td>\n",
       "      <td>thousand croatian celebrate world cup slalom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['u', 'first', 'lady', 'laura', 'bush', 'u', '...</td>\n",
       "      <td>['laura', 'bush', 'unk', 'rice', 'attend', 'si...</td>\n",
       "      <td>u first lady laura bush u secretary state cond...</td>\n",
       "      <td>laura bush unk rice attend sirleaf inauguratio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  ['fivetime', 'world', 'champion', 'michelle', ...   \n",
       "1  ['u', 'business', 'leader', 'lashed', 'wednesd...   \n",
       "2  ['general', 'motor', 'corp', 'said', 'wednesda...   \n",
       "3  ['several', 'thousand', 'people', 'gathered', ...   \n",
       "4  ['u', 'first', 'lady', 'laura', 'bush', 'u', '...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  ['injury', 'leaf', 'kwan', 'olympic', 'hope', ...   \n",
       "1  ['u', 'business', 'attack', 'tough', 'immigrat...   \n",
       "2      ['gm', 'december', 'sale', 'fall', 'percent']   \n",
       "3  ['thousand', 'croatian', 'celebrate', 'world',...   \n",
       "4  ['laura', 'bush', 'unk', 'rice', 'attend', 'si...   \n",
       "\n",
       "                                           docString  \\\n",
       "0  fivetime world champion michelle kwan withdrew...   \n",
       "1  u business leader lashed wednesday legislation...   \n",
       "2  general motor corp said wednesday u sale fell ...   \n",
       "3  several thousand people gathered wednesday eve...   \n",
       "4  u first lady laura bush u secretary state cond...   \n",
       "\n",
       "                                           sumString  \n",
       "0                injury leaf kwan olympic hope limbo  \n",
       "1            u business attack tough immigration law  \n",
       "2                      gm december sale fall percent  \n",
       "3       thousand croatian celebrate world cup slalom  \n",
       "4  laura bush unk rice attend sirleaf inauguratio...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"docString\"] = df_val[\"document\"].map(all2String)\n",
    "df_val[\"sumString\"] = df_val[\"summary\"].map(all2String)\n",
    "df_val.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"index\"] = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = df_train.iloc[0][\"document\"]\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = samp_li.replace(\"\\'\", \"\").strip('][').replace(',', '')\n",
    "# print(type(samp_li))\n",
    "# # samp_li = samp_li.split(', ')\n",
    "# max_len = int(len(samp_li)/2)\n",
    "# print(max_len)\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_art = \" \".join(samp_li)\n",
    "# samp_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_sum = df_train.iloc[0][\"summary\"].replace(\"\\'\", \"\").strip('][').split(', ')\n",
    "# samp_sum = \" \".join(samp_sum)\n",
    "# samp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australia', 'current', 'account', 'deficit', 'shrunk', 'record', 'billion', 'dollar', 'lrb', 'billion', 'u', 'rrb', 'june', 'quarter', 'due', 'soaring', 'commodity', 'price', 'figure', 'released', 'monday', 'showed']\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[0][\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia current account deficit shrunk record billion dollar lrb billion u rrb june quarter due soaring commodity price figure released monday showed\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[0][\"docString\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['australia', 'current', 'account', 'deficit',...</td>\n",
       "      <td>['australian', 'current', 'account', 'deficit'...</td>\n",
       "      <td>australia current account deficit shrunk recor...</td>\n",
       "      <td>australian current account deficit narrow sharply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['least', 'two', 'people', 'killed', 'suspecte...</td>\n",
       "      <td>['least', 'two', 'dead', 'southern', 'philippi...</td>\n",
       "      <td>least two people killed suspected bomb attack ...</td>\n",
       "      <td>least two dead southern philippine blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['australian', 'share', 'closed', 'percent', '...</td>\n",
       "      <td>['australian', 'stock', 'close', 'percent']</td>\n",
       "      <td>australian share closed percent monday followi...</td>\n",
       "      <td>australian stock close percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['south', 'korea', 'nuclear', 'envoy', 'kim', ...</td>\n",
       "      <td>['envoy', 'urge', 'north', 'korea', 'restart',...</td>\n",
       "      <td>south korea nuclear envoy kim sook urged north...</td>\n",
       "      <td>envoy urge north korea restart nuclear disable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['south', 'korea', 'monday', 'announced', 'swe...</td>\n",
       "      <td>['skorea', 'announces', 'tax', 'cut', 'stimula...</td>\n",
       "      <td>south korea monday announced sweeping tax refo...</td>\n",
       "      <td>skorea announces tax cut stimulate economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>['majestic', 'citadel', 'atop', 'syria', 'anci...</td>\n",
       "      <td>['aga', 'khan', 'pours', 'wealth', 'islamic', ...</td>\n",
       "      <td>majestic citadel atop syria ancient city alepp...</td>\n",
       "      <td>aga khan pours wealth islamic site syria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>['eu', 'institutional', 'crisis', 'sparked', '...</td>\n",
       "      <td>['eu', 'losing', 'hope', 'swift', 'solution', ...</td>\n",
       "      <td>eu institutional crisis sparked irish voter re...</td>\n",
       "      <td>eu losing hope swift solution treaty crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>['business', 'brisk', 'rosary', 'palace', 'unk...</td>\n",
       "      <td>['lourdes', 'supermarket', 'soul', 'want', 'ke...</td>\n",
       "      <td>business brisk rosary palace unk pilgrim fill ...</td>\n",
       "      <td>lourdes supermarket soul want keep pilgrim coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>['people', 'expected', 'attend', 'openair', 'm...</td>\n",
       "      <td>['pope', 'make', 'pilgrimage', 'lourdes', 'shr...</td>\n",
       "      <td>people expected attend openair mass given pope...</td>\n",
       "      <td>pope make pilgrimage lourdes shrine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>['nightmare', 'foreign', 'policy', 'expert', '...</td>\n",
       "      <td>['ukraine', 'crimea', 'dream', 'union', 'russia']</td>\n",
       "      <td>nightmare foreign policy expert former soviet ...</td>\n",
       "      <td>ukraine crimea dream union russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  \\\n",
       "0    ['australia', 'current', 'account', 'deficit',...   \n",
       "1    ['least', 'two', 'people', 'killed', 'suspecte...   \n",
       "2    ['australian', 'share', 'closed', 'percent', '...   \n",
       "3    ['south', 'korea', 'nuclear', 'envoy', 'kim', ...   \n",
       "4    ['south', 'korea', 'monday', 'announced', 'swe...   \n",
       "..                                                 ...   \n",
       "995  ['majestic', 'citadel', 'atop', 'syria', 'anci...   \n",
       "996  ['eu', 'institutional', 'crisis', 'sparked', '...   \n",
       "997  ['business', 'brisk', 'rosary', 'palace', 'unk...   \n",
       "998  ['people', 'expected', 'attend', 'openair', 'm...   \n",
       "999  ['nightmare', 'foreign', 'policy', 'expert', '...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    ['australian', 'current', 'account', 'deficit'...   \n",
       "1    ['least', 'two', 'dead', 'southern', 'philippi...   \n",
       "2          ['australian', 'stock', 'close', 'percent']   \n",
       "3    ['envoy', 'urge', 'north', 'korea', 'restart',...   \n",
       "4    ['skorea', 'announces', 'tax', 'cut', 'stimula...   \n",
       "..                                                 ...   \n",
       "995  ['aga', 'khan', 'pours', 'wealth', 'islamic', ...   \n",
       "996  ['eu', 'losing', 'hope', 'swift', 'solution', ...   \n",
       "997  ['lourdes', 'supermarket', 'soul', 'want', 'ke...   \n",
       "998  ['pope', 'make', 'pilgrimage', 'lourdes', 'shr...   \n",
       "999  ['ukraine', 'crimea', 'dream', 'union', 'russia']   \n",
       "\n",
       "                                             docString  \\\n",
       "0    australia current account deficit shrunk recor...   \n",
       "1    least two people killed suspected bomb attack ...   \n",
       "2    australian share closed percent monday followi...   \n",
       "3    south korea nuclear envoy kim sook urged north...   \n",
       "4    south korea monday announced sweeping tax refo...   \n",
       "..                                                 ...   \n",
       "995  majestic citadel atop syria ancient city alepp...   \n",
       "996  eu institutional crisis sparked irish voter re...   \n",
       "997  business brisk rosary palace unk pilgrim fill ...   \n",
       "998  people expected attend openair mass given pope...   \n",
       "999  nightmare foreign policy expert former soviet ...   \n",
       "\n",
       "                                             sumString  \n",
       "0    australian current account deficit narrow sharply  \n",
       "1             least two dead southern philippine blast  \n",
       "2                       australian stock close percent  \n",
       "3    envoy urge north korea restart nuclear disable...  \n",
       "4           skorea announces tax cut stimulate economy  \n",
       "..                                                 ...  \n",
       "995           aga khan pours wealth islamic site syria  \n",
       "996        eu losing hope swift solution treaty crisis  \n",
       "997  lourdes supermarket soul want keep pilgrim coming  \n",
       "998                pope make pilgrimage lourdes shrine  \n",
       "999                  ukraine crimea dream union russia  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = df_train.head(1000)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBart(df):\n",
    "    predictions = []\n",
    "    for i in range(len(df)):\n",
    "        doc = df.iloc[i][\"docString\"]\n",
    "        maxLen = int(len(doc.split(\" \")))\n",
    "        predictions.append(summarizer(doc, max_length=maxLen, min_length=1, do_sample=False)[0][\"summary_text\"])\n",
    "\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heman\\AppData\\Local\\Temp\\ipykernel_16012\\2630650038.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"BART_Pred\"] = predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "      <th>BART_Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['australia', 'current', 'account', 'deficit',...</td>\n",
       "      <td>['australian', 'current', 'account', 'deficit'...</td>\n",
       "      <td>australia current account deficit shrunk recor...</td>\n",
       "      <td>australian current account deficit narrow sharply</td>\n",
       "      <td>australia current account deficit shrunk reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['least', 'two', 'people', 'killed', 'suspecte...</td>\n",
       "      <td>['least', 'two', 'dead', 'southern', 'philippi...</td>\n",
       "      <td>least two people killed suspected bomb attack ...</td>\n",
       "      <td>least two dead southern philippine blast</td>\n",
       "      <td>At least two people killed suspected bomb atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['australian', 'share', 'closed', 'percent', '...</td>\n",
       "      <td>['australian', 'stock', 'close', 'percent']</td>\n",
       "      <td>australian share closed percent monday followi...</td>\n",
       "      <td>australian stock close percent</td>\n",
       "      <td>australian share closed percent monday follow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['south', 'korea', 'nuclear', 'envoy', 'kim', ...</td>\n",
       "      <td>['envoy', 'urge', 'north', 'korea', 'restart',...</td>\n",
       "      <td>south korea nuclear envoy kim sook urged north...</td>\n",
       "      <td>envoy urge north korea restart nuclear disable...</td>\n",
       "      <td>South Korea nuclear envoy kim sook urged north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['south', 'korea', 'monday', 'announced', 'swe...</td>\n",
       "      <td>['skorea', 'announces', 'tax', 'cut', 'stimula...</td>\n",
       "      <td>south korea monday announced sweeping tax refo...</td>\n",
       "      <td>skorea announces tax cut stimulate economy</td>\n",
       "      <td>south korea announced sweeping tax reform incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>['majestic', 'citadel', 'atop', 'syria', 'anci...</td>\n",
       "      <td>['aga', 'khan', 'pours', 'wealth', 'islamic', ...</td>\n",
       "      <td>majestic citadel atop syria ancient city alepp...</td>\n",
       "      <td>aga khan pours wealth islamic site syria</td>\n",
       "      <td>Aga khan promote islamic site. Part of project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>['eu', 'institutional', 'crisis', 'sparked', '...</td>\n",
       "      <td>['eu', 'losing', 'hope', 'swift', 'solution', ...</td>\n",
       "      <td>eu institutional crisis sparked irish voter re...</td>\n",
       "      <td>eu losing hope swift solution treaty crisis</td>\n",
       "      <td>eu institutional crisis sparked irish voter r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>['business', 'brisk', 'rosary', 'palace', 'unk...</td>\n",
       "      <td>['lourdes', 'supermarket', 'soul', 'want', 'ke...</td>\n",
       "      <td>business brisk rosary palace unk pilgrim fill ...</td>\n",
       "      <td>lourdes supermarket soul want keep pilgrim coming</td>\n",
       "      <td>The rosary palace unk pilgrim fill basket glow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>['people', 'expected', 'attend', 'openair', 'm...</td>\n",
       "      <td>['pope', 'make', 'pilgrimage', 'lourdes', 'shr...</td>\n",
       "      <td>people expected attend openair mass given pope...</td>\n",
       "      <td>pope make pilgrimage lourdes shrine</td>\n",
       "      <td>People expected to attend openair mass given p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>['nightmare', 'foreign', 'policy', 'expert', '...</td>\n",
       "      <td>['ukraine', 'crimea', 'dream', 'union', 'russia']</td>\n",
       "      <td>nightmare foreign policy expert former soviet ...</td>\n",
       "      <td>ukraine crimea dream union russia</td>\n",
       "      <td>Former soviet union longcherished dream many l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  \\\n",
       "0    ['australia', 'current', 'account', 'deficit',...   \n",
       "1    ['least', 'two', 'people', 'killed', 'suspecte...   \n",
       "2    ['australian', 'share', 'closed', 'percent', '...   \n",
       "3    ['south', 'korea', 'nuclear', 'envoy', 'kim', ...   \n",
       "4    ['south', 'korea', 'monday', 'announced', 'swe...   \n",
       "..                                                 ...   \n",
       "995  ['majestic', 'citadel', 'atop', 'syria', 'anci...   \n",
       "996  ['eu', 'institutional', 'crisis', 'sparked', '...   \n",
       "997  ['business', 'brisk', 'rosary', 'palace', 'unk...   \n",
       "998  ['people', 'expected', 'attend', 'openair', 'm...   \n",
       "999  ['nightmare', 'foreign', 'policy', 'expert', '...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    ['australian', 'current', 'account', 'deficit'...   \n",
       "1    ['least', 'two', 'dead', 'southern', 'philippi...   \n",
       "2          ['australian', 'stock', 'close', 'percent']   \n",
       "3    ['envoy', 'urge', 'north', 'korea', 'restart',...   \n",
       "4    ['skorea', 'announces', 'tax', 'cut', 'stimula...   \n",
       "..                                                 ...   \n",
       "995  ['aga', 'khan', 'pours', 'wealth', 'islamic', ...   \n",
       "996  ['eu', 'losing', 'hope', 'swift', 'solution', ...   \n",
       "997  ['lourdes', 'supermarket', 'soul', 'want', 'ke...   \n",
       "998  ['pope', 'make', 'pilgrimage', 'lourdes', 'shr...   \n",
       "999  ['ukraine', 'crimea', 'dream', 'union', 'russia']   \n",
       "\n",
       "                                             docString  \\\n",
       "0    australia current account deficit shrunk recor...   \n",
       "1    least two people killed suspected bomb attack ...   \n",
       "2    australian share closed percent monday followi...   \n",
       "3    south korea nuclear envoy kim sook urged north...   \n",
       "4    south korea monday announced sweeping tax refo...   \n",
       "..                                                 ...   \n",
       "995  majestic citadel atop syria ancient city alepp...   \n",
       "996  eu institutional crisis sparked irish voter re...   \n",
       "997  business brisk rosary palace unk pilgrim fill ...   \n",
       "998  people expected attend openair mass given pope...   \n",
       "999  nightmare foreign policy expert former soviet ...   \n",
       "\n",
       "                                             sumString  \\\n",
       "0    australian current account deficit narrow sharply   \n",
       "1             least two dead southern philippine blast   \n",
       "2                       australian stock close percent   \n",
       "3    envoy urge north korea restart nuclear disable...   \n",
       "4           skorea announces tax cut stimulate economy   \n",
       "..                                                 ...   \n",
       "995           aga khan pours wealth islamic site syria   \n",
       "996        eu losing hope swift solution treaty crisis   \n",
       "997  lourdes supermarket soul want keep pilgrim coming   \n",
       "998                pope make pilgrimage lourdes shrine   \n",
       "999                  ukraine crimea dream union russia   \n",
       "\n",
       "                                             BART_Pred  \n",
       "0     australia current account deficit shrunk reco...  \n",
       "1    At least two people killed suspected bomb atta...  \n",
       "2     australian share closed percent monday follow...  \n",
       "3    South Korea nuclear envoy kim sook urged north...  \n",
       "4    south korea announced sweeping tax reform incl...  \n",
       "..                                                 ...  \n",
       "995  Aga khan promote islamic site. Part of project...  \n",
       "996   eu institutional crisis sparked irish voter r...  \n",
       "997  The rosary palace unk pilgrim fill basket glow...  \n",
       "998  People expected to attend openair mass given p...  \n",
       "999  Former soviet union longcherished dream many l...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = runBart(dft)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# for i in range(len(df_train)):\n",
    "#     doc = df_train.iloc[i][\"docString\"]\n",
    "#     # print(doc)\n",
    "#     # print(len(doc.split(\" \")))\n",
    "#     maxlen = int(len(doc.split(\" \")))\n",
    "#     # print(maxlen)\n",
    "#     # pred = summarizer(doc, max_length=maxlen, min_length=4, do_sample=False)[0][\"summary_text\"]\n",
    "#     # print(pred)\n",
    "#     predictions.append(summarizer(doc, max_length=maxlen, min_length=1, do_sample=False)[0][\"summary_text\"])\n",
    "    \n",
    "# df_train[\"Prediction\"] = predictions\n",
    "# df_train\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = summarizer(samp_art, max_length=max_len, min_length=4, do_sample=False)\n",
    "# res = res[0][\"summary_text\"]\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install evaluate\n",
    "# ! pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dft[\"BART_Pred\"]\n",
    "references = dft[\"sumString\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(results.keys())\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision is 0.7917156093120575\n",
      "Average recall is 0.8347355498075485\n",
      "Average f1 is 0.8123201101422309\n"
     ]
    }
   ],
   "source": [
    "keys = list(results.keys())\n",
    "for k in range(len(keys)-1):\n",
    "    s = sum(results[keys[k]])\n",
    "    le = len(results[keys[k]])\n",
    "    avg = s/le\n",
    "    print(\"Average {} is {}\".format(keys[k], avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = res\n",
    "references = samp_sum\n",
    "results = rouge.compute(predictions=[predictions], references=[references])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = samp_sum.split(\" \")\n",
    "r = res.split(\" \")\n",
    "print(ss)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = round(meteor([r, ss], r), 4)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = res.split(\" \")\n",
    "references = samp_sum.split(\" \")\n",
    "results = bleu.compute(predictions=[[predictions]], references=[[references]])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics - BERTScore and ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "\n",
    "\n",
    "We have chosen to use BERTScore and ROGUE as our two metrics for evaluating the performance of our model. We chose to use ROGUE metrics because it is purpose-built for evaluating text summarization, which is the task our model will be completing. We will be generating scores for ROGUE-N (as ROGUE-1 and ROGUE-2), ROUGE-L, and ROUGE-S for the most comprehensive possible model evaluation. ROGUE-1 and ROGUE-2 will observe the number of unigrams and bigrams (respectively) shared between the model output and the \"correct\" output. ROGUE-L measures the longest common subsequence of words shared between the model's output and the true output. ROGUE-S observes shared skipgrams between the model's output and the desired one, this can identify sequences of consecutive words that may be correct in the model's output but are separated by a word or sequence of words. These metrics will provide a method by which to assign accuracy, precision, recall, and F1 scores when comparing the model's produced summaries with the original human-generated ones.\n",
    "\n",
    "We chose to use BERTScore as our second metric because it is another metric that is designed to evaluate how a model's text output compares with a true output. We thought it would be interesting to pair a BERTScore evaluation with our ROGUE evaluations because BERTScore, unlike ROGUE or BLEU, focuses on a semantic comparison of the model's output and the original output, rather than a purely syntactical one. This means, rather than computing a pure accuracy score in terms of how many exact words are matched between the true and model outputs, BERTScore takes into account the meaning of individual words in each output when making evaluations. This can make for an analysis that may be more in line with human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building our own Model if needed:\n",
    "https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Pre-Trained Models from Huggingface: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
