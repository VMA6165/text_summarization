{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Model Baseline\n",
    "### DSBA 6165\n",
    "### Divam Arora, Connor Moore, Hemanth Velan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "* https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 3803957\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 1951\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 189651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df exported.\n",
      "Test df exported.\n",
      "Validation df exported.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the train-test split\n",
    "The standard provided division between train, test, and validation is extremely unbalanced towards train (95%), and the dataset overall is far too large to run through our model in a reasonable timespan. We decided to shrink the train set to 70,000 entries, and concat the provided test and validation sets. From that combined test-val set we will extract a 25,000-entry test set and a 5,000 entry validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>a british soldier was killed saturday by an ex...</td>\n",
       "      <td>british soldier killed in afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>ukraine insists on building two new nuclear re...</td>\n",
       "      <td>ukraine insists on linking chernobyl closure t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>portuguese president mario soares will pay an ...</td>\n",
       "      <td>portugal 's president to visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>aol stepped up its transformation from interne...</td>\n",
       "      <td>aol introduces new advertising network plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>marine experts from wwf flew to the northern k...</td>\n",
       "      <td>suspected toxic algae bloom leaves thousands o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124694</th>\n",
       "      <td>hong kong 's benchmark hang seng index ended h...</td>\n",
       "      <td>hong kong stocks edged up after four straight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237703</th>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira says he 's close to an agreement to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671101</th>\n",
       "      <td>around ## youths on thursday protested outside...</td>\n",
       "      <td>latvian youths protest ban of UNK symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601285</th>\n",
       "      <td>ohio 's method of putting prisoners to death i...</td>\n",
       "      <td>ohio judge says state s lethal injection proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716533</th>\n",
       "      <td>yi zhang and lin xing made a #-# chinese finis...</td>\n",
       "      <td>china 's yi wins women 's triathlon at asian b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   a british soldier was killed saturday by an ex...   \n",
       "1506983  ukraine insists on building two new nuclear re...   \n",
       "3429980  portuguese president mario soares will pay an ...   \n",
       "2028209  aol stepped up its transformation from interne...   \n",
       "1392922  marine experts from wwf flew to the northern k...   \n",
       "...                                                    ...   \n",
       "3124694  hong kong 's benchmark hang seng index ended h...   \n",
       "1237703  former brazil coach carlos alberto parreira sa...   \n",
       "671101   around ## youths on thursday protested outside...   \n",
       "1601285  ohio 's method of putting prisoners to death i...   \n",
       "3716533  yi zhang and lin xing made a #-# chinese finis...   \n",
       "\n",
       "                                                   summary  \n",
       "644708         british soldier killed in afghanistan blast  \n",
       "1506983  ukraine insists on linking chernobyl closure t...  \n",
       "3429980   portugal 's president to visit angola next month  \n",
       "2028209  aol introduces new advertising network plans t...  \n",
       "1392922  suspected toxic algae bloom leaves thousands o...  \n",
       "...                                                    ...  \n",
       "3124694  hong kong stocks edged up after four straight ...  \n",
       "1237703  parreira says he 's close to an agreement to c...  \n",
       "671101           latvian youths protest ban of UNK symbols  \n",
       "1601285  ohio judge says state s lethal injection proce...  \n",
       "3716533  china 's yi wins women 's triathlon at asian b...  \n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 70,000 rows randomly from the train dataframe\n",
    "\n",
    "df_train_short = df_train.sample(n = 70000, random_state=2)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine provided test and val sets and reseparate randomly into smaller subsets\n",
    "\n",
    "# concat test and validation sets\n",
    "test_val = [df_test, df_val]\n",
    "df_testval_bulk = pd.concat(test_val)\n",
    "\n",
    "# take a random sample of 30000 rows from the test and validation bulk set\n",
    "df_testval_short = df_testval_bulk.sample(n = 30000, random_state=3)\n",
    "\n",
    "# take a random 5000 row sample from the test-val subset\n",
    "df_val_short = df_testval_short.sample(n = 5000, random_state=4)\n",
    "\n",
    "# drop all rows taken for the validation sample from the test-val subset to create the test set\n",
    "df_test_short = df_testval_short.drop(df_val_short.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=True)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=True)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code and ideas for this process were gathered from the following geeks for geeks page and article -\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "# comments and functions are original work, source code was modified to fit our workspace\n",
    "\n",
    "# tokenization and removal of stopwords\n",
    "\n",
    "# create an instance of all stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function for removing stopwords from a given input\n",
    "def remove_stopwords(text):\n",
    "    # tokenize the input string\n",
    "    tokens = word_tokenize(text)\n",
    "    # create an empty list for the new output\n",
    "    filtered_tokens = []\n",
    "    # for each word in the tokenized text\n",
    "    for word in tokens:\n",
    "        # if the word is not a stop word\n",
    "        if word not in stop_words:\n",
    "            # add the token to the new output list\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# function to apply the stopword removal/tokenization function to input dataframes\n",
    "def tokenize_nostop(df):\n",
    "    # iterate through the dataframe and tokenize/remove stop words for each row\n",
    "    df[\"document\"] = df[\"document\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from document column\")\n",
    "    \n",
    "    df[\"summary\"] = df[\"summary\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from summary column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration and source code for NLTK's word net lemmatizer came from the following article -\n",
    "# https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "# functions and comments are our original work, source code was modified to fit our workspace\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "# create an instance of NLTK's word net lemmatizer class\n",
    "wml = WordNetLemmatizer()\n",
    "\n",
    "# function to lemmatize a given tokenized text input\n",
    "def lemmatization(text):\n",
    "    # create an empty list for new output\n",
    "    lemma_words = []\n",
    "\n",
    "    # for each word in the given input\n",
    "    for word in text:\n",
    "        # lemmatize the word\n",
    "        token = wml.lemmatize(word)\n",
    "        # and add it into our new output list\n",
    "        lemma_words.append(token)\n",
    "    \n",
    "    return lemma_words\n",
    "\n",
    "# function to call lemmatization function on the rows of an input dataframe\n",
    "def lemmatize(df):\n",
    "    # iterate through the rows of the input dataframe and apply the lemmatization function to each row\n",
    "    df[\"document\"] = df[\"document\"].apply(lemmatization)\n",
    "    print(\"document column lemmatized\")\n",
    "\n",
    "    df[\"summary\"] = df[\"summary\"].apply(lemmatization)\n",
    "    print(\"summary column lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    # tokenize and remove stopwords\n",
    "    tokenize_nostop(df)\n",
    "    # lemmatize\n",
    "    lemmatize(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "train df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "test df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "validation df completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887130</th>\n",
       "      <td>[indian, share, price, closed, percent, higher...</td>\n",
       "      <td>[indian, share, close, pct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230066</th>\n",
       "      <td>[gustav, slammed, cuba, tobaccogrowing, wester...</td>\n",
       "      <td>[gustav, slam, cuba, massive, category, hurric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933565</th>\n",
       "      <td>[week, ago, researcher, wisconsin, japan, said...</td>\n",
       "      <td>[unk, stem, cell, venture, land, million]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592999</th>\n",
       "      <td>[two, japan, biggest, soccer, star, returned, ...</td>\n",
       "      <td>[soccer, star, return, home, dropped, national...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960389</th>\n",
       "      <td>[united, state, britain, unleashed, massive, a...</td>\n",
       "      <td>[u, unleashes, aerial, assault, take, port, ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "887130   [indian, share, price, closed, percent, higher...   \n",
       "1230066  [gustav, slammed, cuba, tobaccogrowing, wester...   \n",
       "2933565  [week, ago, researcher, wisconsin, japan, said...   \n",
       "1592999  [two, japan, biggest, soccer, star, returned, ...   \n",
       "960389   [united, state, britain, unleashed, massive, a...   \n",
       "\n",
       "                                                   summary  \n",
       "644708      [british, soldier, killed, afghanistan, blast]  \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...  \n",
       "3429980  [portugal, president, visit, angola, next, month]  \n",
       "2028209  [aol, introduces, new, advertising, network, p...  \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...  \n",
       "887130                         [indian, share, close, pct]  \n",
       "1230066  [gustav, slam, cuba, massive, category, hurric...  \n",
       "2933565          [unk, stem, cell, venture, land, million]  \n",
       "1592999  [soccer, star, return, home, dropped, national...  \n",
       "960389   [u, unleashes, aerial, assault, take, port, ai...  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "pre_proc(df_train_short)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_val_short)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_test_short)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train_short.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert lists to string format for improved model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list entries into single strings\n",
    "\n",
    "def list2string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "      <td>british soldier killed saturday explosion sout...</td>\n",
       "      <td>british soldier killed afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "      <td>ukraine insists linking chernobyl closure buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "      <td>portugal president visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "      <td>aol introduces new advertising network plan mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "      <td>suspected toxic algae bloom leaf thousand fish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "\n",
       "                                                   summary  \\\n",
       "644708      [british, soldier, killed, afghanistan, blast]   \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...   \n",
       "3429980  [portugal, president, visit, angola, next, month]   \n",
       "2028209  [aol, introduces, new, advertising, network, p...   \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...   \n",
       "\n",
       "                                                 docString  \\\n",
       "644708   british soldier killed saturday explosion sout...   \n",
       "1506983  ukraine insists building two new nuclear react...   \n",
       "3429980  portuguese president mario soares pay official...   \n",
       "2028209  aol stepped transformation internet access pro...   \n",
       "1392922  marine expert wwf flew northern kenyan coast t...   \n",
       "\n",
       "                                                 sumString  \n",
       "644708            british soldier killed afghanistan blast  \n",
       "1506983  ukraine insists linking chernobyl closure buil...  \n",
       "3429980         portugal president visit angola next month  \n",
       "2028209  aol introduces new advertising network plan mo...  \n",
       "1392922  suspected toxic algae bloom leaf thousand fish...  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply function to create new stringified columns for\n",
    "\n",
    "# train\n",
    "df_train_short[\"docString\"] = df_train_short[\"document\"].map(list2string)\n",
    "df_train_short[\"sumString\"] = df_train_short[\"summary\"].map(list2string)\n",
    "\n",
    "# test\n",
    "df_test_short[\"docString\"] = df_test_short[\"document\"].map(list2string)\n",
    "df_test_short[\"sumString\"] = df_test_short[\"summary\"].map(list2string)\n",
    "\n",
    "# and val\n",
    "df_val_short[\"docString\"] = df_val_short[\"document\"].map(list2string)\n",
    "df_val_short[\"sumString\"] = df_val_short[\"summary\"].map(list2string)\n",
    "\n",
    "df_train_short.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying something different\n",
    "\n",
    "https://blog.paperspace.com/bart-model-for-text-summarization-part1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model loading for bart-base\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBart(df):\n",
    "    predictions = []\n",
    "    times = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        start = time.perf_counter()\n",
    "        doc = df.iloc[i][\"docString\"]\n",
    "        maxLen = int(len(doc) / 10)\n",
    "\n",
    "        # Transmitting the encoded inputs to the model.generate() function\n",
    "        inputs = tokenizer.batch_encode_plus([doc],return_tensors='pt')\n",
    "        summary_ids =  model.generate(inputs['input_ids'], max_length=maxLen, min_length=0)\n",
    "\n",
    "        # Decoding and printing the summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(summary)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        speed = end - start\n",
    "        times.append(speed)\n",
    "        if i % 1000 == 0:\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(\"average time per row at\", i, \"row: \", avg_time)\n",
    "\n",
    "\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runBart(df_train_short)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan is to try and incorporate that^ model as a starting point into some kind of training/epoch/class/loop thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"index\"] = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = df_train.iloc[0][\"document\"]\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = samp_li.replace(\"\\'\", \"\").strip('][').replace(',', '')\n",
    "# print(type(samp_li))\n",
    "# # samp_li = samp_li.split(', ')\n",
    "# max_len = int(len(samp_li)/2)\n",
    "# print(max_len)\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_art = \" \".join(samp_li)\n",
    "# samp_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_sum = df_train.iloc[0][\"summary\"].replace(\"\\'\", \"\").strip('][').split(', ')\n",
    "# samp_sum = \" \".join(samp_sum)\n",
    "# samp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[0][\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[0][\"docString\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df_train.head(1000)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBart(df):\n",
    "    predictions = []\n",
    "    times = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        start = time.perf_counter()\n",
    "        doc = df.iloc[i][\"docString\"]\n",
    "        maxLen = int(len(doc) / 10)\n",
    "        summary = summarizer(doc, max_length=maxLen,  min_length=1, do_sample=False)[0][\"summary_text\"]\n",
    "        predictions.append(summary)\n",
    "        end = time.perf_counter()\n",
    "        speed = end - start\n",
    "        times.append(speed)\n",
    "        if i % 25 == 0:\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(\"average time per row at\", i, \"row: \", avg_time)\n",
    "\n",
    "\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_short = runBart(df_train_short)\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# for i in range(len(df_train)):\n",
    "#     doc = df_train.iloc[i][\"docString\"]\n",
    "#     # print(doc)\n",
    "#     # print(len(doc.split(\" \")))\n",
    "#     maxlen = int(len(doc.split(\" \")))\n",
    "#     # print(maxlen)\n",
    "#     # pred = summarizer(doc, max_length=maxlen, min_length=4, do_sample=False)[0][\"summary_text\"]\n",
    "#     # print(pred)\n",
    "#     predictions.append(summarizer(doc, max_length=maxlen, min_length=1, do_sample=False)[0][\"summary_text\"])\n",
    "    \n",
    "# df_train[\"Prediction\"] = predictions\n",
    "# df_train\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = summarizer(samp_art, max_length=max_len, min_length=4, do_sample=False)\n",
    "# res = res[0][\"summary_text\"]\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install evaluate\n",
    "# ! pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import load\n",
    "# bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = df_train_short[\"BART_Pred\"]\n",
    "# references = df_train_short[\"sumString\"]\n",
    "# results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "# # print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(results.keys())\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(results.keys())\n",
    "# for k in range(len(keys)-1):\n",
    "#     s = sum(results[keys[k]])\n",
    "#     le = len(results[keys[k]])\n",
    "#     avg = s/le\n",
    "#     print(\"Average {} is {}\".format(keys[k], avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "# rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = res\n",
    "# references = samp_sum\n",
    "# results = rouge.compute(predictions=[predictions], references=[references])\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate import meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = samp_sum.split(\" \")\n",
    "# r = res.split(\" \")\n",
    "# print(ss)\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = round(meteor([r, ss], r), 4)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = res.split(\" \")\n",
    "# references = samp_sum.split(\" \")\n",
    "# results = bleu.compute(predictions=[[predictions]], references=[[references]])\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics - BERTScore and ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "\n",
    "\n",
    "We have chosen to use BERTScore and ROGUE as our two metrics for evaluating the performance of our model. We chose to use ROGUE metrics because it is purpose-built for evaluating text summarization, which is the task our model will be completing. We will be generating scores for ROGUE-N (as ROGUE-1 and ROGUE-2), ROUGE-L, and ROUGE-S for the most comprehensive possible model evaluation. ROGUE-1 and ROGUE-2 will observe the number of unigrams and bigrams (respectively) shared between the model output and the \"correct\" output. ROGUE-L measures the longest common subsequence of words shared between the model's output and the true output. ROGUE-S observes shared skipgrams between the model's output and the desired one, this can identify sequences of consecutive words that may be correct in the model's output but are separated by a word or sequence of words. These metrics will provide a method by which to assign accuracy, precision, recall, and F1 scores when comparing the model's produced summaries with the original human-generated ones.\n",
    "\n",
    "We chose to use BERTScore as our second metric because it is another metric that is designed to evaluate how a model's text output compares with a true output. We thought it would be interesting to pair a BERTScore evaluation with our ROGUE evaluations because BERTScore, unlike ROGUE or BLEU, focuses on a semantic comparison of the model's output and the original output, rather than a purely syntactical one. This means, rather than computing a pure accuracy score in terms of how many exact words are matched between the true and model outputs, BERTScore takes into account the meaning of individual words in each output when making evaluations. This can make for an analysis that may be more in line with human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building our own Model if needed:\n",
    "https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Pre-Trained Models from Huggingface: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why bart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
