{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Model Baseline\n",
    "### DSBA 6165\n",
    "### Divam Arora, Connor Moore, Hemanth Velan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "* https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !conda install -c huggingface transformers\n",
    "# !pip install transformers==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import pipeline, BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 3803957\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 1951\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 189651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df exported.\n",
      "Test df exported.\n",
      "Validation df exported.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the train-test split\n",
    "The standard provided division between train, test, and validation is extremely unbalanced towards train (95%), and the dataset overall is far too large to run through our model in a reasonable timespan. We decided to shrink the train set to 70,000 entries, and concat the provided test and validation sets. From that combined test-val set we will extract a 25,000-entry test set and a 5,000 entry validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>a british soldier was killed saturday by an ex...</td>\n",
       "      <td>british soldier killed in afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>ukraine insists on building two new nuclear re...</td>\n",
       "      <td>ukraine insists on linking chernobyl closure t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>portuguese president mario soares will pay an ...</td>\n",
       "      <td>portugal 's president to visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>aol stepped up its transformation from interne...</td>\n",
       "      <td>aol introduces new advertising network plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>marine experts from wwf flew to the northern k...</td>\n",
       "      <td>suspected toxic algae bloom leaves thousands o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124694</th>\n",
       "      <td>hong kong 's benchmark hang seng index ended h...</td>\n",
       "      <td>hong kong stocks edged up after four straight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237703</th>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira says he 's close to an agreement to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671101</th>\n",
       "      <td>around ## youths on thursday protested outside...</td>\n",
       "      <td>latvian youths protest ban of UNK symbols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601285</th>\n",
       "      <td>ohio 's method of putting prisoners to death i...</td>\n",
       "      <td>ohio judge says state s lethal injection proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716533</th>\n",
       "      <td>yi zhang and lin xing made a #-# chinese finis...</td>\n",
       "      <td>china 's yi wins women 's triathlon at asian b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   a british soldier was killed saturday by an ex...   \n",
       "1506983  ukraine insists on building two new nuclear re...   \n",
       "3429980  portuguese president mario soares will pay an ...   \n",
       "2028209  aol stepped up its transformation from interne...   \n",
       "1392922  marine experts from wwf flew to the northern k...   \n",
       "...                                                    ...   \n",
       "3124694  hong kong 's benchmark hang seng index ended h...   \n",
       "1237703  former brazil coach carlos alberto parreira sa...   \n",
       "671101   around ## youths on thursday protested outside...   \n",
       "1601285  ohio 's method of putting prisoners to death i...   \n",
       "3716533  yi zhang and lin xing made a #-# chinese finis...   \n",
       "\n",
       "                                                   summary  \n",
       "644708         british soldier killed in afghanistan blast  \n",
       "1506983  ukraine insists on linking chernobyl closure t...  \n",
       "3429980   portugal 's president to visit angola next month  \n",
       "2028209  aol introduces new advertising network plans t...  \n",
       "1392922  suspected toxic algae bloom leaves thousands o...  \n",
       "...                                                    ...  \n",
       "3124694  hong kong stocks edged up after four straight ...  \n",
       "1237703  parreira says he 's close to an agreement to c...  \n",
       "671101           latvian youths protest ban of UNK symbols  \n",
       "1601285  ohio judge says state s lethal injection proce...  \n",
       "3716533  china 's yi wins women 's triathlon at asian b...  \n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select 70,000 rows randomly from the train dataframe\n",
    "\n",
    "df_train_short = df_train.sample(n = 70000, random_state=2)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine provided test and val sets and reseparate randomly into smaller subsets\n",
    "\n",
    "# concat test and validation sets\n",
    "test_val = [df_test, df_val]\n",
    "df_testval_bulk = pd.concat(test_val)\n",
    "\n",
    "# take a random sample of 30000 rows from the test and validation bulk set\n",
    "df_testval_short = df_testval_bulk.sample(n = 30000, random_state=3)\n",
    "\n",
    "# take a random 5000 row sample from the test-val subset\n",
    "df_val_short = df_testval_short.sample(n = 5000, random_state=4)\n",
    "\n",
    "# drop all rows taken for the validation sample from the test-val subset to create the test set\n",
    "df_test_short = df_testval_short.drop(df_val_short.index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=False)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=False)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code and ideas for this process were gathered from the following geeks for geeks page and article -\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "# comments and functions are original work, source code was modified to fit our workspace\n",
    "\n",
    "# tokenization and removal of stopwords\n",
    "\n",
    "# create an instance of all stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function for removing stopwords from a given input\n",
    "def remove_stopwords(text):\n",
    "    # tokenize the input string\n",
    "    tokens = word_tokenize(text)\n",
    "    # create an empty list for the new output\n",
    "    filtered_tokens = []\n",
    "    # for each word in the tokenized text\n",
    "    for word in tokens:\n",
    "        # if the word is not a stop word\n",
    "        if word not in stop_words:\n",
    "            # add the token to the new output list\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# function to apply the stopword removal/tokenization function to input dataframes\n",
    "def tokenize_nostop(df):\n",
    "    # iterate through the dataframe and tokenize/remove stop words for each row\n",
    "    df[\"document\"] = df[\"document\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from document column\")\n",
    "    \n",
    "    df[\"summary\"] = df[\"summary\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from summary column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration and source code for NLTK's word net lemmatizer came from the following article -\n",
    "# https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "# functions and comments are our original work, source code was modified to fit our workspace\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "# create an instance of NLTK's word net lemmatizer class\n",
    "wml = WordNetLemmatizer()\n",
    "\n",
    "# function to lemmatize a given tokenized text input\n",
    "def lemmatization(text):\n",
    "    # create an empty list for new output\n",
    "    lemma_words = []\n",
    "\n",
    "    # for each word in the given input\n",
    "    for word in text:\n",
    "        # lemmatize the word\n",
    "        token = wml.lemmatize(word)\n",
    "        # and add it into our new output list\n",
    "        lemma_words.append(token)\n",
    "    \n",
    "    return lemma_words\n",
    "\n",
    "# function to call lemmatization function on the rows of an input dataframe\n",
    "def lemmatize(df):\n",
    "    # iterate through the rows of the input dataframe and apply the lemmatization function to each row\n",
    "    df[\"document\"] = df[\"document\"].apply(lemmatization)\n",
    "    print(\"document column lemmatized\")\n",
    "\n",
    "    df[\"summary\"] = df[\"summary\"].apply(lemmatization)\n",
    "    print(\"summary column lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    # tokenize and remove stopwords\n",
    "    tokenize_nostop(df)\n",
    "    # lemmatize\n",
    "    lemmatize(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "train df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "test df completed\n",
      "summary column lowercased\n",
      "document column lowercased\n",
      "symbols removed\n",
      "stopwords removed from document column\n",
      "stopwords removed from summary column\n",
      "document column lemmatized\n",
      "summary column lemmatized\n",
      "pre-processed successfully\n",
      "validation df completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887130</th>\n",
       "      <td>[indian, share, price, closed, percent, higher...</td>\n",
       "      <td>[indian, share, close, pct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230066</th>\n",
       "      <td>[gustav, slammed, cuba, tobaccogrowing, wester...</td>\n",
       "      <td>[gustav, slam, cuba, massive, category, hurric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933565</th>\n",
       "      <td>[week, ago, researcher, wisconsin, japan, said...</td>\n",
       "      <td>[unk, stem, cell, venture, land, million]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592999</th>\n",
       "      <td>[two, japan, biggest, soccer, star, returned, ...</td>\n",
       "      <td>[soccer, star, return, home, dropped, national...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960389</th>\n",
       "      <td>[united, state, britain, unleashed, massive, a...</td>\n",
       "      <td>[u, unleashes, aerial, assault, take, port, ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "887130   [indian, share, price, closed, percent, higher...   \n",
       "1230066  [gustav, slammed, cuba, tobaccogrowing, wester...   \n",
       "2933565  [week, ago, researcher, wisconsin, japan, said...   \n",
       "1592999  [two, japan, biggest, soccer, star, returned, ...   \n",
       "960389   [united, state, britain, unleashed, massive, a...   \n",
       "\n",
       "                                                   summary  \n",
       "644708      [british, soldier, killed, afghanistan, blast]  \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...  \n",
       "3429980  [portugal, president, visit, angola, next, month]  \n",
       "2028209  [aol, introduces, new, advertising, network, p...  \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...  \n",
       "887130                         [indian, share, close, pct]  \n",
       "1230066  [gustav, slam, cuba, massive, category, hurric...  \n",
       "2933565          [unk, stem, cell, venture, land, million]  \n",
       "1592999  [soccer, star, return, home, dropped, national...  \n",
       "960389   [u, unleashes, aerial, assault, take, port, ai...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "pre_proc(df_train_short)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_val_short)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_test_short)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train_short.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert lists to string format for improved model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list entries into single strings\n",
    "\n",
    "def list2string(input):\n",
    "    output = \" \".join(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "      <td>british soldier killed saturday explosion sout...</td>\n",
       "      <td>british soldier killed afghanistan blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "      <td>ukraine insists linking chernobyl closure buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "      <td>portugal president visit angola next month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "      <td>aol introduces new advertising network plan mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "      <td>suspected toxic algae bloom leaf thousand fish...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "\n",
       "                                                   summary  \\\n",
       "644708      [british, soldier, killed, afghanistan, blast]   \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...   \n",
       "3429980  [portugal, president, visit, angola, next, month]   \n",
       "2028209  [aol, introduces, new, advertising, network, p...   \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...   \n",
       "\n",
       "                                                 docString  \\\n",
       "644708   british soldier killed saturday explosion sout...   \n",
       "1506983  ukraine insists building two new nuclear react...   \n",
       "3429980  portuguese president mario soares pay official...   \n",
       "2028209  aol stepped transformation internet access pro...   \n",
       "1392922  marine expert wwf flew northern kenyan coast t...   \n",
       "\n",
       "                                                 sumString  \n",
       "644708            british soldier killed afghanistan blast  \n",
       "1506983  ukraine insists linking chernobyl closure buil...  \n",
       "3429980         portugal president visit angola next month  \n",
       "2028209  aol introduces new advertising network plan mo...  \n",
       "1392922  suspected toxic algae bloom leaf thousand fish...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply function to create new stringified columns for\n",
    "\n",
    "# train\n",
    "df_train_short[\"docString\"] = df_train_short[\"document\"].map(list2string)\n",
    "df_train_short[\"sumString\"] = df_train_short[\"summary\"].map(list2string)\n",
    "\n",
    "# test\n",
    "df_test_short[\"docString\"] = df_test_short[\"document\"].map(list2string)\n",
    "df_test_short[\"sumString\"] = df_test_short[\"summary\"].map(list2string)\n",
    "\n",
    "# and val\n",
    "df_val_short[\"docString\"] = df_val_short[\"document\"].map(list2string)\n",
    "df_val_short[\"sumString\"] = df_val_short[\"summary\"].map(list2string)\n",
    "\n",
    "df_train_short.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying something different\n",
    "\n",
    "https://blog.paperspace.com/bart-model-for-text-summarization-part1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb085d4a7a894eee855ddd773355fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heman\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\heman\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model loading for bart-base\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Cuda Toolkit 12.1.0\n",
    "# Download Pytorch Cuda from this website:\n",
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBart(df):\n",
    "    predictions = []\n",
    "    times = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        start = time.perf_counter()\n",
    "        doc = df.iloc[i][\"docString\"]\n",
    "        maxLen = int(len(doc) / 10)+1\n",
    "\n",
    "        # Transmitting the encoded inputs to the model.generate() function\n",
    "        inputs = tokenizer.batch_encode_plus([doc],return_tensors='pt').to(device)\n",
    "        summary_ids =  model.generate(inputs['input_ids'], max_length=maxLen, min_length=0)\n",
    "\n",
    "        # Decoding and printing the summary\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(summary)\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        speed = end - start\n",
    "        times.append(speed)\n",
    "        if i % 1000 == 0:\n",
    "            avg_time = sum(times) / len(times)\n",
    "            print(\"average time per row at\", i, \"row: \", avg_time)\n",
    "\n",
    "\n",
    "    df[\"BART_Pred\"] = predictions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average time per row at 0 row:  1.3844849000161048\n",
      "average time per row at 1000 row:  0.13752335014969477\n",
      "average time per row at 2000 row:  0.13661073253370531\n",
      "average time per row at 3000 row:  0.14155803655433416\n",
      "average time per row at 4000 row:  0.14157741497109072\n",
      "average time per row at 5000 row:  0.13809193395304983\n",
      "average time per row at 6000 row:  0.13655051448093058\n",
      "average time per row at 7000 row:  0.1351348340237436\n",
      "average time per row at 8000 row:  0.13427754680675216\n",
      "average time per row at 9000 row:  0.1335037070548648\n",
      "average time per row at 10000 row:  0.13289672331774152\n",
      "average time per row at 11000 row:  0.13216971444427067\n",
      "average time per row at 12000 row:  0.1316068232898574\n",
      "average time per row at 13000 row:  0.1314000820630301\n",
      "average time per row at 14000 row:  0.13102607890876516\n",
      "average time per row at 15000 row:  0.131005642337324\n",
      "average time per row at 16000 row:  0.1307042999439079\n",
      "average time per row at 17000 row:  0.13076432159887147\n",
      "average time per row at 18000 row:  0.13042574478096644\n",
      "average time per row at 19000 row:  0.13030972746183336\n",
      "average time per row at 20000 row:  0.13028650645479115\n",
      "average time per row at 21000 row:  0.13011329929058194\n",
      "average time per row at 22000 row:  0.12997746811967711\n",
      "average time per row at 23000 row:  0.13087455398031872\n",
      "average time per row at 24000 row:  0.13310867963423062\n",
      "average time per row at 25000 row:  0.1364228175353632\n",
      "average time per row at 26000 row:  0.13946140080004663\n",
      "average time per row at 27000 row:  0.1419281039147403\n",
      "average time per row at 28000 row:  0.144124959905099\n",
      "average time per row at 29000 row:  0.14433557333549232\n",
      "average time per row at 30000 row:  0.14391806594456694\n",
      "average time per row at 31000 row:  0.14353522880240172\n",
      "average time per row at 32000 row:  0.14320697796952617\n",
      "average time per row at 33000 row:  0.142884136268689\n",
      "average time per row at 34000 row:  0.14291758039478175\n",
      "average time per row at 35000 row:  0.1465377919460237\n",
      "average time per row at 36000 row:  0.15591777035648366\n",
      "average time per row at 37000 row:  0.16266378537617976\n",
      "average time per row at 38000 row:  0.161904020773258\n",
      "average time per row at 39000 row:  0.1612289393196202\n",
      "average time per row at 40000 row:  0.16053801032736012\n",
      "average time per row at 41000 row:  0.1598824494745182\n",
      "average time per row at 42000 row:  0.15998763211600098\n",
      "average time per row at 43000 row:  0.161158572007724\n",
      "average time per row at 44000 row:  0.16233872825626505\n",
      "average time per row at 45000 row:  0.16300695037895593\n",
      "average time per row at 46000 row:  0.16232701841053265\n",
      "average time per row at 47000 row:  0.16166385542655032\n",
      "average time per row at 48000 row:  0.1610344012583843\n",
      "average time per row at 49000 row:  0.1604526459134186\n",
      "average time per row at 50000 row:  0.15990949549015168\n",
      "average time per row at 51000 row:  0.15934816484780795\n",
      "average time per row at 52000 row:  0.15882011840931667\n",
      "average time per row at 53000 row:  0.1582939658025888\n",
      "average time per row at 54000 row:  0.15774840418146394\n",
      "average time per row at 55000 row:  0.1572481529172848\n",
      "average time per row at 56000 row:  0.1567684620828859\n",
      "average time per row at 57000 row:  0.15625366538311905\n",
      "average time per row at 58000 row:  0.15570731442911534\n",
      "average time per row at 59000 row:  0.1552502342350599\n",
      "average time per row at 60000 row:  0.15480396422064321\n",
      "average time per row at 61000 row:  0.15436419557225659\n",
      "average time per row at 62000 row:  0.15387057539883092\n",
      "average time per row at 63000 row:  0.15342430325872336\n",
      "average time per row at 64000 row:  0.15295790247187585\n",
      "average time per row at 65000 row:  0.15255198892944916\n",
      "average time per row at 66000 row:  0.15216442059217156\n",
      "average time per row at 67000 row:  0.1517703258384766\n",
      "average time per row at 68000 row:  0.15140466675936795\n",
      "average time per row at 69000 row:  0.15105172535909983\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "      <th>BART_Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "      <td>british soldier killed saturday explosion sout...</td>\n",
       "      <td>british soldier killed afghanistan blast</td>\n",
       "      <td>british soldier killed saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "      <td>ukraine insists linking chernobyl closure buil...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "      <td>portugal president visit angola next month</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "      <td>aol introduces new advertising network plan mo...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "      <td>suspected toxic algae bloom leaf thousand fish...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124694</th>\n",
       "      <td>[hong, kong, benchmark, hang, seng, index, end...</td>\n",
       "      <td>[hong, kong, stock, edged, four, straight, ses...</td>\n",
       "      <td>hong kong benchmark hang seng index ended high...</td>\n",
       "      <td>hong kong stock edged four straight session dec</td>\n",
       "      <td>hong kong benchmark hang seng index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237703</th>\n",
       "      <td>[former, brazil, coach, carlos, alberto, parre...</td>\n",
       "      <td>[parreira, say, close, agreement, coach, south...</td>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira say close agreement coach south africa</td>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671101</th>\n",
       "      <td>[around, youth, thursday, protested, outside, ...</td>\n",
       "      <td>[latvian, youth, protest, ban, unk, symbol]</td>\n",
       "      <td>around youth thursday protested outside latvia...</td>\n",
       "      <td>latvian youth protest ban unk symbol</td>\n",
       "      <td>around youth thursday protested outside latvia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601285</th>\n",
       "      <td>[ohio, method, putting, prisoner, death, uncon...</td>\n",
       "      <td>[ohio, judge, say, state, lethal, injection, p...</td>\n",
       "      <td>ohio method putting prisoner death unconstitut...</td>\n",
       "      <td>ohio judge say state lethal injection process ...</td>\n",
       "      <td>ohio method putting prisoner death unconstitut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716533</th>\n",
       "      <td>[yi, zhang, lin, xing, made, chinese, finish, ...</td>\n",
       "      <td>[china, yi, win, woman, triathlon, asian, beac...</td>\n",
       "      <td>yi zhang lin xing made chinese finish woman tr...</td>\n",
       "      <td>china yi win woman triathlon asian beach game</td>\n",
       "      <td>yi zhang lin x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "...                                                    ...   \n",
       "3124694  [hong, kong, benchmark, hang, seng, index, end...   \n",
       "1237703  [former, brazil, coach, carlos, alberto, parre...   \n",
       "671101   [around, youth, thursday, protested, outside, ...   \n",
       "1601285  [ohio, method, putting, prisoner, death, uncon...   \n",
       "3716533  [yi, zhang, lin, xing, made, chinese, finish, ...   \n",
       "\n",
       "                                                   summary  \\\n",
       "644708      [british, soldier, killed, afghanistan, blast]   \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...   \n",
       "3429980  [portugal, president, visit, angola, next, month]   \n",
       "2028209  [aol, introduces, new, advertising, network, p...   \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...   \n",
       "...                                                    ...   \n",
       "3124694  [hong, kong, stock, edged, four, straight, ses...   \n",
       "1237703  [parreira, say, close, agreement, coach, south...   \n",
       "671101         [latvian, youth, protest, ban, unk, symbol]   \n",
       "1601285  [ohio, judge, say, state, lethal, injection, p...   \n",
       "3716533  [china, yi, win, woman, triathlon, asian, beac...   \n",
       "\n",
       "                                                 docString  \\\n",
       "644708   british soldier killed saturday explosion sout...   \n",
       "1506983  ukraine insists building two new nuclear react...   \n",
       "3429980  portuguese president mario soares pay official...   \n",
       "2028209  aol stepped transformation internet access pro...   \n",
       "1392922  marine expert wwf flew northern kenyan coast t...   \n",
       "...                                                    ...   \n",
       "3124694  hong kong benchmark hang seng index ended high...   \n",
       "1237703  former brazil coach carlos alberto parreira sa...   \n",
       "671101   around youth thursday protested outside latvia...   \n",
       "1601285  ohio method putting prisoner death unconstitut...   \n",
       "3716533  yi zhang lin xing made chinese finish woman tr...   \n",
       "\n",
       "                                                 sumString  \\\n",
       "644708            british soldier killed afghanistan blast   \n",
       "1506983  ukraine insists linking chernobyl closure buil...   \n",
       "3429980         portugal president visit angola next month   \n",
       "2028209  aol introduces new advertising network plan mo...   \n",
       "1392922  suspected toxic algae bloom leaf thousand fish...   \n",
       "...                                                    ...   \n",
       "3124694    hong kong stock edged four straight session dec   \n",
       "1237703    parreira say close agreement coach south africa   \n",
       "671101                latvian youth protest ban unk symbol   \n",
       "1601285  ohio judge say state lethal injection process ...   \n",
       "3716533      china yi win woman triathlon asian beach game   \n",
       "\n",
       "                                                 BART_Pred  \n",
       "644708                     british soldier killed saturday  \n",
       "1506983  ukraine insists building two new nuclear react...  \n",
       "3429980  portuguese president mario soares pay official...  \n",
       "2028209  aol stepped transformation internet access pro...  \n",
       "1392922  marine expert wwf flew northern kenyan coast t...  \n",
       "...                                                    ...  \n",
       "3124694                hong kong benchmark hang seng index  \n",
       "1237703  former brazil coach carlos alberto parreira sa...  \n",
       "671101   around youth thursday protested outside latvia...  \n",
       "1601285  ohio method putting prisoner death unconstitut...  \n",
       "3716533                                     yi zhang lin x  \n",
       "\n",
       "[70000 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runBart(df_train_short)\n",
    "\n",
    "df_train_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plan is to try and incorporate that^ model as a starting point into some kind of training/epoch/class/loop thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"index\"] = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = df_train.iloc[0][\"document\"]\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_li = samp_li.replace(\"\\'\", \"\").strip('][').replace(',', '')\n",
    "# print(type(samp_li))\n",
    "# # samp_li = samp_li.split(', ')\n",
    "# max_len = int(len(samp_li)/2)\n",
    "# print(max_len)\n",
    "# samp_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_art = \" \".join(samp_li)\n",
    "# samp_art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samp_sum = df_train.iloc[0][\"summary\"].replace(\"\\'\", \"\").strip('][').split(', ')\n",
    "# samp_sum = \" \".join(samp_sum)\n",
    "# samp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dont Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train.iloc[0][\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train.iloc[0][\"docString\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = df_train.head(1000)\n",
    "# dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def runBart(df):\n",
    "#     predictions = []\n",
    "#     times = []\n",
    "\n",
    "#     for i in range(len(df)):\n",
    "#         start = time.perf_counter()\n",
    "#         doc = df.iloc[i][\"docString\"]\n",
    "#         maxLen = int(len(doc) / 10)\n",
    "#         summary = summarizer(doc, max_length=maxLen,  min_length=1, do_sample=False)[0][\"summary_text\"]\n",
    "#         predictions.append(summary)\n",
    "#         end = time.perf_counter()\n",
    "#         speed = end - start\n",
    "#         times.append(speed)\n",
    "#         if i % 25 == 0:\n",
    "#             avg_time = sum(times) / len(times)\n",
    "#             print(\"average time per row at\", i, \"row: \", avg_time)\n",
    "\n",
    "\n",
    "#     df[\"BART_Pred\"] = predictions\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_short = runBart(df_train_short)\n",
    "# df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# for i in range(len(df_train)):\n",
    "#     doc = df_train.iloc[i][\"docString\"]\n",
    "#     # print(doc)\n",
    "#     # print(len(doc.split(\" \")))\n",
    "#     maxlen = int(len(doc.split(\" \")))\n",
    "#     # print(maxlen)\n",
    "#     # pred = summarizer(doc, max_length=maxlen, min_length=4, do_sample=False)[0][\"summary_text\"]\n",
    "#     # print(pred)\n",
    "#     predictions.append(summarizer(doc, max_length=maxlen, min_length=1, do_sample=False)[0][\"summary_text\"])\n",
    "    \n",
    "# df_train[\"Prediction\"] = predictions\n",
    "# df_train\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = summarizer(samp_art, max_length=max_len, min_length=4, do_sample=False)\n",
    "# res = res[0][\"summary_text\"]\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install evaluate\n",
    "# ! pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>docString</th>\n",
       "      <th>sumString</th>\n",
       "      <th>BART_Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>644708</th>\n",
       "      <td>[british, soldier, killed, saturday, explosion...</td>\n",
       "      <td>[british, soldier, killed, afghanistan, blast]</td>\n",
       "      <td>british soldier killed saturday explosion sout...</td>\n",
       "      <td>british soldier killed afghanistan blast</td>\n",
       "      <td>british soldier killed saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506983</th>\n",
       "      <td>[ukraine, insists, building, two, new, nuclear...</td>\n",
       "      <td>[ukraine, insists, linking, chernobyl, closure...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "      <td>ukraine insists linking chernobyl closure buil...</td>\n",
       "      <td>ukraine insists building two new nuclear react...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429980</th>\n",
       "      <td>[portuguese, president, mario, soares, pay, of...</td>\n",
       "      <td>[portugal, president, visit, angola, next, month]</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "      <td>portugal president visit angola next month</td>\n",
       "      <td>portuguese president mario soares pay official...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028209</th>\n",
       "      <td>[aol, stepped, transformation, internet, acces...</td>\n",
       "      <td>[aol, introduces, new, advertising, network, p...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "      <td>aol introduces new advertising network plan mo...</td>\n",
       "      <td>aol stepped transformation internet access pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392922</th>\n",
       "      <td>[marine, expert, wwf, flew, northern, kenyan, ...</td>\n",
       "      <td>[suspected, toxic, algae, bloom, leaf, thousan...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "      <td>suspected toxic algae bloom leaf thousand fish...</td>\n",
       "      <td>marine expert wwf flew northern kenyan coast t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124694</th>\n",
       "      <td>[hong, kong, benchmark, hang, seng, index, end...</td>\n",
       "      <td>[hong, kong, stock, edged, four, straight, ses...</td>\n",
       "      <td>hong kong benchmark hang seng index ended high...</td>\n",
       "      <td>hong kong stock edged four straight session dec</td>\n",
       "      <td>hong kong benchmark hang seng index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237703</th>\n",
       "      <td>[former, brazil, coach, carlos, alberto, parre...</td>\n",
       "      <td>[parreira, say, close, agreement, coach, south...</td>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "      <td>parreira say close agreement coach south africa</td>\n",
       "      <td>former brazil coach carlos alberto parreira sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671101</th>\n",
       "      <td>[around, youth, thursday, protested, outside, ...</td>\n",
       "      <td>[latvian, youth, protest, ban, unk, symbol]</td>\n",
       "      <td>around youth thursday protested outside latvia...</td>\n",
       "      <td>latvian youth protest ban unk symbol</td>\n",
       "      <td>around youth thursday protested outside latvia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601285</th>\n",
       "      <td>[ohio, method, putting, prisoner, death, uncon...</td>\n",
       "      <td>[ohio, judge, say, state, lethal, injection, p...</td>\n",
       "      <td>ohio method putting prisoner death unconstitut...</td>\n",
       "      <td>ohio judge say state lethal injection process ...</td>\n",
       "      <td>ohio method putting prisoner death unconstitut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716533</th>\n",
       "      <td>[yi, zhang, lin, xing, made, chinese, finish, ...</td>\n",
       "      <td>[china, yi, win, woman, triathlon, asian, beac...</td>\n",
       "      <td>yi zhang lin xing made chinese finish woman tr...</td>\n",
       "      <td>china yi win woman triathlon asian beach game</td>\n",
       "      <td>yi zhang lin x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "644708   [british, soldier, killed, saturday, explosion...   \n",
       "1506983  [ukraine, insists, building, two, new, nuclear...   \n",
       "3429980  [portuguese, president, mario, soares, pay, of...   \n",
       "2028209  [aol, stepped, transformation, internet, acces...   \n",
       "1392922  [marine, expert, wwf, flew, northern, kenyan, ...   \n",
       "...                                                    ...   \n",
       "3124694  [hong, kong, benchmark, hang, seng, index, end...   \n",
       "1237703  [former, brazil, coach, carlos, alberto, parre...   \n",
       "671101   [around, youth, thursday, protested, outside, ...   \n",
       "1601285  [ohio, method, putting, prisoner, death, uncon...   \n",
       "3716533  [yi, zhang, lin, xing, made, chinese, finish, ...   \n",
       "\n",
       "                                                   summary  \\\n",
       "644708      [british, soldier, killed, afghanistan, blast]   \n",
       "1506983  [ukraine, insists, linking, chernobyl, closure...   \n",
       "3429980  [portugal, president, visit, angola, next, month]   \n",
       "2028209  [aol, introduces, new, advertising, network, p...   \n",
       "1392922  [suspected, toxic, algae, bloom, leaf, thousan...   \n",
       "...                                                    ...   \n",
       "3124694  [hong, kong, stock, edged, four, straight, ses...   \n",
       "1237703  [parreira, say, close, agreement, coach, south...   \n",
       "671101         [latvian, youth, protest, ban, unk, symbol]   \n",
       "1601285  [ohio, judge, say, state, lethal, injection, p...   \n",
       "3716533  [china, yi, win, woman, triathlon, asian, beac...   \n",
       "\n",
       "                                                 docString  \\\n",
       "644708   british soldier killed saturday explosion sout...   \n",
       "1506983  ukraine insists building two new nuclear react...   \n",
       "3429980  portuguese president mario soares pay official...   \n",
       "2028209  aol stepped transformation internet access pro...   \n",
       "1392922  marine expert wwf flew northern kenyan coast t...   \n",
       "...                                                    ...   \n",
       "3124694  hong kong benchmark hang seng index ended high...   \n",
       "1237703  former brazil coach carlos alberto parreira sa...   \n",
       "671101   around youth thursday protested outside latvia...   \n",
       "1601285  ohio method putting prisoner death unconstitut...   \n",
       "3716533  yi zhang lin xing made chinese finish woman tr...   \n",
       "\n",
       "                                                 sumString  \\\n",
       "644708            british soldier killed afghanistan blast   \n",
       "1506983  ukraine insists linking chernobyl closure buil...   \n",
       "3429980         portugal president visit angola next month   \n",
       "2028209  aol introduces new advertising network plan mo...   \n",
       "1392922  suspected toxic algae bloom leaf thousand fish...   \n",
       "...                                                    ...   \n",
       "3124694    hong kong stock edged four straight session dec   \n",
       "1237703    parreira say close agreement coach south africa   \n",
       "671101                latvian youth protest ban unk symbol   \n",
       "1601285  ohio judge say state lethal injection process ...   \n",
       "3716533      china yi win woman triathlon asian beach game   \n",
       "\n",
       "                                                 BART_Pred  \n",
       "644708                     british soldier killed saturday  \n",
       "1506983  ukraine insists building two new nuclear react...  \n",
       "3429980  portuguese president mario soares pay official...  \n",
       "2028209  aol stepped transformation internet access pro...  \n",
       "1392922  marine expert wwf flew northern kenyan coast t...  \n",
       "...                                                    ...  \n",
       "3124694                hong kong benchmark hang seng index  \n",
       "1237703  former brazil coach carlos alberto parreira sa...  \n",
       "671101   around youth thursday protested outside latvia...  \n",
       "1601285  ohio method putting prisoner death unconstitut...  \n",
       "3716533                                     yi zhang lin x  \n",
       "\n",
       "[70000 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_short[\"BART_Pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_short[\"sumString\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\heman\\Desktop\\Github\\text_summarization\\groupproject_model_baseline_new.ipynb Cell 55\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m df_train_short[\u001b[39m\"\u001b[39m\u001b[39mBART_Pred\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m references \u001b[39m=\u001b[39m df_train_short[\u001b[39m\"\u001b[39m\u001b[39msumString\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m bertscore\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mreferences, model_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\evaluate\\module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 450\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_batch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\evaluate\\module.py:509\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m batch \u001b[39m=\u001b[39m {input_name: batch[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n\u001b[0;32m    511\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\evaluate\\module.py:590\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m    589\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 590\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\evaluate\\module.py:590\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m    589\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 590\u001b[0m     example \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(k, v[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()])\n\u001b[0;32m    591\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions = df_train_short[\"BART_Pred\"]\n",
    "references = df_train_short[\"sumString\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(results.keys())\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = list(results.keys())\n",
    "# for k in range(len(keys)-1):\n",
    "#     s = sum(results[keys[k]])\n",
    "#     le = len(results[keys[k]])\n",
    "#     avg = s/le\n",
    "#     print(\"Average {} is {}\".format(keys[k], avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_short' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\heman\\Desktop\\Github\\text_summarization\\groupproject_model_baseline_new.ipynb Cell 64\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m df_train_short[\u001b[39m\"\u001b[39m\u001b[39mBART_Pred\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m references \u001b[39m=\u001b[39m df_train_short[\u001b[39m\"\u001b[39m\u001b[39msumString\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heman/Desktop/Github/text_summarization/groupproject_model_baseline_new.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m results \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39m[predictions], references\u001b[39m=\u001b[39m[references])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train_short' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = df_train_short[\"BART_Pred\"]\n",
    "references = df_train_short[\"sumString\"]\n",
    "results = rouge.compute(predictions=[predictions], references=[references])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate import meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = samp_sum.split(\" \")\n",
    "# r = res.split(\" \")\n",
    "# print(ss)\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = round(meteor([r, ss], r), 4)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu = load_metric(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = res.split(\" \")\n",
    "# references = samp_sum.split(\" \")\n",
    "# results = bleu.compute(predictions=[[predictions]], references=[[references]])\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics - BERTScore and ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "\n",
    "\n",
    "We have chosen to use BERTScore and ROGUE as our two metrics for evaluating the performance of our model. We chose to use ROGUE metrics because it is purpose-built for evaluating text summarization, which is the task our model will be completing. We will be generating scores for ROGUE-N (as ROGUE-1 and ROGUE-2), ROUGE-L, and ROUGE-S for the most comprehensive possible model evaluation. ROGUE-1 and ROGUE-2 will observe the number of unigrams and bigrams (respectively) shared between the model output and the \"correct\" output. ROGUE-L measures the longest common subsequence of words shared between the model's output and the true output. ROGUE-S observes shared skipgrams between the model's output and the desired one, this can identify sequences of consecutive words that may be correct in the model's output but are separated by a word or sequence of words. These metrics will provide a method by which to assign accuracy, precision, recall, and F1 scores when comparing the model's produced summaries with the original human-generated ones.\n",
    "\n",
    "We chose to use BERTScore as our second metric because it is another metric that is designed to evaluate how a model's text output compares with a true output. We thought it would be interesting to pair a BERTScore evaluation with our ROGUE evaluations because BERTScore, unlike ROGUE or BLEU, focuses on a semantic comparison of the model's output and the original output, rather than a purely syntactical one. This means, rather than computing a pure accuracy score in terms of how many exact words are matched between the true and model outputs, BERTScore takes into account the meaning of individual words in each output when making evaluations. This can make for an analysis that may be more in line with human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building our own Model if needed:\n",
    "https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning Pre-Trained Models from Huggingface: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# why bart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
