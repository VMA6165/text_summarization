{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project - Model Baseline\n",
    "### DSBA 6165\n",
    "### Divam Arora, Connor Moore, Hemanth Velan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "* https://huggingface.co/datasets/gigaword\n",
    "* https://huggingface.co/docs/datasets/process#export\n",
    "* https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "* https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "* https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "* https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "* https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "* https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "* https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "* https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "* https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "* https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "* https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to re-run the code from our EDA/pre-processing notebook that loads and prepares our dataset for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmoor197\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import needed packages\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# download stop word package from nltk library\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that because the dataset we used is sourced directly from HuggingFace's library, there is no need for a local copy of the data. Running this notebook automatically downloads and initializes the dataset and allows it to be used as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/gigaword\n",
    "# https://huggingface.co/docs/datasets/v1.11.0/splits.html\n",
    "\n",
    "# download gigaword dataset from Hugging Face dataset library\n",
    "train, test, validation = ds.load_dataset(\"gigaword\", split=[\"train\", \"test\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataset splits\n",
    "print(train)\n",
    "print(test)\n",
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/process#export\n",
    "\n",
    "# export the training dataset to a pandas dataframe and display\n",
    "df_train = train.to_pandas()\n",
    "print(\"Train df exported.\")\n",
    "\n",
    "# export the test dataset to a pandas dataframe\n",
    "df_test = test.to_pandas()\n",
    "print(\"Test df exported.\")\n",
    "\n",
    "# export the validation dataset to a pandas dataframe\n",
    "df_val = validation.to_pandas()\n",
    "print(\"Validation df exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the methods required to perform this function were found in this article -\n",
    "# https://aparnamishra144.medium.com/how-to-change-string-data-or-text-data-of-a-column-to-lowercase-in-pandas-248a8ce4ae01\n",
    "# the function and comments are our original work\n",
    "\n",
    "# set all words in all rows to lower case\n",
    "\n",
    "def lower(df):\n",
    "    # vectorize strings in each row in summary column and set to lower case\n",
    "    df[\"summary\"] = df[\"summary\"].str.lower()\n",
    "    print(\"summary column lowercased\")\n",
    "    # vectorize strings in each row in document column and set to lower case\n",
    "    df[\"document\"] = df[\"document\"].str.lower()\n",
    "    print(\"document column lowercased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geeks for geeks and pandas doc pages were used as template source code and informed about parameter options\n",
    "# stackoverflow posts helped with debugging issues\n",
    "# https://stackoverflow.com/questions/42135409/removing-a-character-from-entire-data-frame\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "# https://www.geeksforgeeks.org/string-punctuation-in-python/\n",
    "# https://stackoverflow.com/questions/41425945/python-pandas-error-missing-unterminated-subpattern-at-position-2\n",
    "# https://stackoverflow.com/questions/28986489/how-to-replace-text-in-a-string-column-of-a-pandas-dataframe\n",
    "# comments and function are our original work, source code was modifed to fit our workspace\n",
    "\n",
    "# remove all symbols and punctuation\n",
    "\n",
    "# create instance of all punctuation symbols\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# since we learned there are lots of apostrophe s in the dataset during EDA, we will add this to our remove list\n",
    "punct_list = [\"'s\"]\n",
    "\n",
    "# add all punctuation from the premade variable to our new list\n",
    "for symbol in punctuation:\n",
    "    punct_list.append(symbol)\n",
    "\n",
    "# display the symbols included in our list\n",
    "print(punct_list)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    # for each symbol in our punctuation list\n",
    "    for symbol in punct_list:\n",
    "        # iterate through the dataframe and replace every instance of the symbol with an empty string\n",
    "        df[\"document\"] = df[\"document\"].str.replace(symbol, \"\", regex=True)\n",
    "        df[\"summary\"] = df[\"summary\"].str.replace(symbol, \"\", regex=True)\n",
    "    print(\"symbols removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code and ideas for this process were gathered from the following geeks for geeks page and article -\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "# comments and functions are original work, source code was modified to fit our workspace\n",
    "\n",
    "# tokenization and removal of stopwords\n",
    "\n",
    "# create an instance of all stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function for removing stopwords from a given input\n",
    "def remove_stopwords(text):\n",
    "    # tokenize the input string\n",
    "    tokens = word_tokenize(text)\n",
    "    # create an empty list for the new output\n",
    "    filtered_tokens = []\n",
    "    # for each word in the tokenized text\n",
    "    for word in tokens:\n",
    "        # if the word is not a stop word\n",
    "        if word not in stop_words:\n",
    "            # add the token to the new output list\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# function to apply the stopword removal/tokenization function to input dataframes\n",
    "def tokenize_nostop(df):\n",
    "    # iterate through the dataframe and tokenize/remove stop words for each row\n",
    "    df[\"document\"] = df[\"document\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from document column\")\n",
    "    \n",
    "    df[\"summary\"] = df[\"summary\"].apply(remove_stopwords)\n",
    "    print(\"stopwords removed from summary column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspiration and source code for NLTK's word net lemmatizer came from the following article -\n",
    "# https://www.analyticsvidhya.com/blog/2021/06/pre-processing-of-text-data-in-nlp/\n",
    "# functions and comments are our original work, source code was modified to fit our workspace\n",
    "\n",
    "# lemmatization\n",
    "\n",
    "# create an instance of NLTK's word net lemmatizer class\n",
    "wml = WordNetLemmatizer()\n",
    "\n",
    "# function to lemmatize a given tokenized text input\n",
    "def lemmatization(text):\n",
    "    # create an empty list for new output\n",
    "    lemma_words = []\n",
    "\n",
    "    # for each word in the given input\n",
    "    for word in text:\n",
    "        # lemmatize the word\n",
    "        token = wml.lemmatize(word)\n",
    "        # and add it into our new output list\n",
    "        lemma_words.append(token)\n",
    "    \n",
    "    return lemma_words\n",
    "\n",
    "# function to call lemmatization function on the rows of an input dataframe\n",
    "def lemmatize(df):\n",
    "    # iterate through the rows of the input dataframe and apply the lemmatization function to each row\n",
    "    df[\"document\"] = df[\"document\"].apply(lemmatization)\n",
    "    print(\"document column lemmatized\")\n",
    "\n",
    "    df[\"summary\"] = df[\"summary\"].apply(lemmatization)\n",
    "    print(\"summary column lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data pre-processing pipeline\n",
    "\n",
    "def pre_proc(df):\n",
    "    # lowercase\n",
    "    lower(df)\n",
    "    # remove punctuation and symbols\n",
    "    remove_punctuation(df)\n",
    "    # tokenize and remove stopwords\n",
    "    tokenize_nostop(df)\n",
    "    # lemmatize\n",
    "    lemmatize(df)\n",
    "    print(\"pre-processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the data pre-processing pipeline for each of the dataset splits\n",
    "\n",
    "pre_proc(df_train)\n",
    "print(\"train df completed\")\n",
    "pre_proc(df_test)\n",
    "print(\"test df completed\")\n",
    "pre_proc(df_val)\n",
    "print(\"validation df completed\")\n",
    "\n",
    "# display new format of data using training set\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset splits are now pre-processed and ready for use with models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set imported\n",
      "test set imported\n",
      "validation set imported\n"
     ]
    }
   ],
   "source": [
    "# import dataframes from saved csv files\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "print(\"train set imported\")\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "print(\"test set imported\")\n",
    "\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "print(\"validation set imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['australia', 'current', 'account', 'deficit',...</td>\n",
       "      <td>['australian', 'current', 'account', 'deficit'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['least', 'two', 'people', 'killed', 'suspecte...</td>\n",
       "      <td>['least', 'two', 'dead', 'southern', 'philippi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['australian', 'share', 'closed', 'percent', '...</td>\n",
       "      <td>['australian', 'stock', 'close', 'percent']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['south', 'korea', 'nuclear', 'envoy', 'kim', ...</td>\n",
       "      <td>['envoy', 'urge', 'north', 'korea', 'restart',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['south', 'korea', 'monday', 'announced', 'swe...</td>\n",
       "      <td>['skorea', 'announces', 'tax', 'cut', 'stimula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803952</th>\n",
       "      <td>['state', 'duma', 'lower', 'house', 'russian',...</td>\n",
       "      <td>['duma', 'urge', 'yeltsin', 'reconsider', 'tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803953</th>\n",
       "      <td>['u', 'justice', 'department', 'today', 'rejec...</td>\n",
       "      <td>['u', 'justice', 'department', 'reject', 'spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803954</th>\n",
       "      <td>['united', 'nation', 'calling', 'million', 'do...</td>\n",
       "      <td>['un', 'seek', 'fund', 'program', 'former', 'y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803955</th>\n",
       "      <td>['president', 'jacques', 'chirac', 'today', 'p...</td>\n",
       "      <td>['chirac', 'get', 'birthday', 'gift', 'th', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803956</th>\n",
       "      <td>['united', 'nation', 'tribunal', 'war', 'crime...</td>\n",
       "      <td>['war', 'crime', 'tribunal', 'pass', 'st', 'se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3803957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  document  \\\n",
       "0        ['australia', 'current', 'account', 'deficit',...   \n",
       "1        ['least', 'two', 'people', 'killed', 'suspecte...   \n",
       "2        ['australian', 'share', 'closed', 'percent', '...   \n",
       "3        ['south', 'korea', 'nuclear', 'envoy', 'kim', ...   \n",
       "4        ['south', 'korea', 'monday', 'announced', 'swe...   \n",
       "...                                                    ...   \n",
       "3803952  ['state', 'duma', 'lower', 'house', 'russian',...   \n",
       "3803953  ['u', 'justice', 'department', 'today', 'rejec...   \n",
       "3803954  ['united', 'nation', 'calling', 'million', 'do...   \n",
       "3803955  ['president', 'jacques', 'chirac', 'today', 'p...   \n",
       "3803956  ['united', 'nation', 'tribunal', 'war', 'crime...   \n",
       "\n",
       "                                                   summary  \n",
       "0        ['australian', 'current', 'account', 'deficit'...  \n",
       "1        ['least', 'two', 'dead', 'southern', 'philippi...  \n",
       "2              ['australian', 'stock', 'close', 'percent']  \n",
       "3        ['envoy', 'urge', 'north', 'korea', 'restart',...  \n",
       "4        ['skorea', 'announces', 'tax', 'cut', 'stimula...  \n",
       "...                                                    ...  \n",
       "3803952  ['duma', 'urge', 'yeltsin', 'reconsider', 'tro...  \n",
       "3803953  ['u', 'justice', 'department', 'reject', 'spec...  \n",
       "3803954  ['un', 'seek', 'fund', 'program', 'former', 'y...  \n",
       "3803955  ['chirac', 'get', 'birthday', 'gift', 'th', 'c...  \n",
       "3803956  ['war', 'crime', 'tribunal', 'pass', 'st', 'se...  \n",
       "\n",
       "[3803957 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics - BERTScore and ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499\n",
    "https://towardsdatascience.com/bertscore-evaluating-text-generation-with-bert-beb7b3431300\n",
    "\n",
    "\n",
    "We have chosen to use BERTScore and ROGUE as our two metrics for evaluating the performance of our model. We chose to use ROGUE metrics because it is purpose-built for evaluating text summarization, which is the task our model will be completing. We will be generating scores for ROGUE-N (as ROGUE-1 and ROGUE-2), ROUGE-L, and ROUGE-S for the most comprehensive possible model evaluation. ROGUE-1 and ROGUE-2 will observe the number of unigrams and bigrams (respectively) shared between the model output and the \"correct\" output. ROGUE-L measures the longest common subsequence of words shared between the model's output and the true output. ROGUE-S observes shared skipgrams between the model's output and the desired one, this can identify sequences of consecutive words that may be correct in the model's output but are separated by a word or sequence of words. These metrics will provide a method by which to assign accuracy, precision, recall, and F1 scores when comparing the model's produced summaries with the original human-generated ones.\n",
    "\n",
    "We chose to use BERTScore as our second metric because it is another metric that is designed to evaluate how a model's text output compares with a true output. We thought it would be interesting to pair a BERTScore evaluation with our ROGUE evaluations because BERTScore, unlike ROGUE or BLEU, focuses on a semantic comparison of the model's output and the original output, rather than a purely syntactical one. This means, rather than computing a pure accuracy score in terms of how many exact words are matched between the true and model outputs, BERTScore takes into account the meaning of individual words in each output when making evaluations. This can make for an analysis that may be more in line with human intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explanation of the model we chose and why its appropriate for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "australia current account deficit shrunk record billion dollar lrb billion u rrb june quarter due soaring commodity price figure released monday showed\n"
     ]
    }
   ],
   "source": [
    "# USE THIS TO STRIP THE \"STRING\" LISTS DOWN TO BARE STRINGS\n",
    "\n",
    "\n",
    "practice = df_train.iloc[0][\"document\"]\n",
    "\n",
    "no_quote = practice[1:-1].replace(\"'\", \"\")\n",
    "\n",
    "no_comm = no_quote.replace(\",\", \"\")\n",
    "\n",
    "print(no_comm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
