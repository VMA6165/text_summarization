{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART model for text summarization\n",
    "\n",
    "We have chosen to use the Bidirectional Auto-Regressive Transformer (BART) model for our text summarization task. The BART model is a sequence-to-sequence transformer model, utilizing a bidirectional encoder and a left to right decoder, that has been shown to be very effective at processing one body of text and outputting different text in response. Bart learns by \"noising\" (scrambling/shuffling/generally introducing chaos to) a given body of text, and training the seq2seq element of the model to recreate the original text. This allows the seq2seq process to learn the unique semantics, flow, grammar, and construction of the given language, allowing the model to produce (roughly) coherent, novel outputs in the given language in response to presented tasks. This makes BART a strong choice for text summarization, and there are several pre-trained variations of BART that are specialized for this task. In addition to manually lemmatizing and tokenizing the string entries in our dataset during our data preprocessing stage, we decided to also utilize BART's pre-trained built-in tokenizer, converting the data into a vectorized form the model can easily interpret, with the hope of optimizing the model's function. For the actual summarization task, we are employing the base pre-trained form of the BART model. We wanted to use the least-trained form of the model for our baseline performance test, so there would be room for optimization and training during the next \"advanced model\" stage of the project.\n",
    "\n",
    "### Sources - \n",
    "* https://www.width.ai/post/bart-text-summarization\n",
    "* https://www.projectpro.io/article/transformers-bart-model-explained/553\n",
    "* https://blog.paperspace.com/bart-model-for-text-summarization-part1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Because our dataset is pulled directly from Huggingface's datasets library, there is no need for a local copy of the data. Running the cell below creates an instance of the specified dataset in your workspace environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sources that might be helpful for the model fine tuning/training stage\n",
    "\n",
    "https://towardsdatascience.com/fine-tuning-the-bart-large-model-for-text-summarization-3c69e4c04582\n",
    "\n",
    "https://keras.io/examples/nlp/abstractive_summarization_with_bart/\n",
    "\n",
    "https://blog.paperspace.com/bart-model-for-text-summarization-part1/\n",
    "\n",
    "https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef\n",
    "\n",
    "https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb\n",
    "\n",
    "https://huggingface.co/docs/transformers/training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
